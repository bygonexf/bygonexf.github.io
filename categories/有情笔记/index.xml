<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>有情笔记 on PIKA☆NCHI</title>
    <link>https://imfaye.me/categories/%E6%9C%89%E6%83%85%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 有情笔记 on PIKA☆NCHI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Faye</copyright>
    <lastBuildDate>Thu, 10 Feb 2022 23:43:59 +0000</lastBuildDate><atom:link href="https://imfaye.me/categories/%E6%9C%89%E6%83%85%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>耶，VAE</title>
      <link>https://imfaye.me/post/vae/</link>
      <pubDate>Thu, 10 Feb 2022 23:43:59 +0000</pubDate>
      
      <guid>https://imfaye.me/post/vae/</guid>
      <description>引子 生成模型 能从可学习的概率分布中采样得到样本的模型。
在一些生成模型中，样本通过将随机的隐层变量送入网络生成得到。
自编码器 AE 自编码器通过学习从输入到隐层和从隐层到输出的映射来重建信号/图像。
目标：$X&amp;rsquo; = D_\theta(E_\phi(X)) \approx X$
$\mathop{min}\limits_{\theta, \phi} \sum\limits_{i=1}^n||D_\theta(E_\phi(X_i))-X_i||^2$，其中 ${{X_i}}_{i=1\cdots n}$ 为数据集。
自编码器并不是一种生成模型，因为它并没有定义一个概率分布，无法采样。
自编码器 → 生成模型？
我们会有一个很自然的做生成模型的想法，那就是训练一个从低维隐层变量生成观测样本的生成模型，最大化观测数据似然。
假设这个生成模型为 $G_{\theta}:\mathbb{R}^k \rightarrow \mathbb{R}^d$，其中 $k &amp;lt; d$，将隐层变量 $Z$ 映射为样本 $X$，那么其实在样本空间里几乎大部分区域 $p(X)=0$。
如果我们从样本空间看，在这个高维空间只会有非常小的一个低维空间子集 $p(X)$ 是有值的，并且我们在训练的时候其实是不知道这个子集的分布的，而其余大部分区域 $p(X)=0$，也就意味着我们很难直接优化似然。
但是有一种方法可以让我们在每一处都得到非零值，那就是在已有先验 $p(Z)$ 的条件下，定义一个有噪声的观测模型 $p_\theta(X|Z)=\mathcal{N}(X;G_\theta(Z), \eta I)$ (其中 $\eta$ 是可调整的参数，$I$ 是单位矩阵)。
所以 $p(X) = \int p(Z)p(X|Z)\mathrm{d}Z$，这个值也很难去计算，所以我们不是去优化 $p(X)$ 而是去优化 $p(X)$ 的下限（变分推断里的证据下限 ELBO，后面会证明）。
那么这其实就是 VAE 的雏形了。
变分自编码器 VAE 上面的图是 VAE 的整体思路，生成的部分也是也就是 decoder 的部分，我们会假设 $Z$ 服从一个简单的先验分布 $p(Z)$，这个分布可以是一个标准正态分布。通过 decoder 会得到高维图像空间的一个概率分布。</description>
    </item>
    
  </channel>
</rss>
