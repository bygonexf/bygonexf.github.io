<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>有情笔记 on 好雪片片</title>
    <link>https://imfaye.me/categories/%E6%9C%89%E6%83%85%E7%AC%94%E8%AE%B0/</link>
    <description>Recent content in 有情笔记 on 好雪片片</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Faye</copyright>
    <lastBuildDate>Sun, 10 Jul 2022 23:43:59 +0000</lastBuildDate><atom:link href="https://imfaye.me/categories/%E6%9C%89%E6%83%85%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>耶，VAE</title>
      <link>https://imfaye.me/post/vae/</link>
      <pubDate>Sun, 10 Jul 2022 23:43:59 +0000</pubDate>
      
      <guid>https://imfaye.me/post/vae/</guid>
      <description>引子 生成模型 能从可学习的概率分布中采样得到样本的模型。
在一些生成模型中，样本通过将随机的隐层变量送入网络生成得到。
自编码器 AE 自编码器通过学习从输入到隐层和从隐层到输出的映射来重建信号/图像。
目标：$X&amp;rsquo; = D_\theta(E_\phi(X)) \approx X$
$\mathop{min}\limits_{\theta, \phi} \sum\limits_{i=1}^n||D_\theta(E_\phi(X_i))-X_i||^2$，其中 ${{X_i}}_{i=1\cdots n}$ 为数据集。
自编码器并不是一种生成模型，因为它并没有定义一个概率分布，无法采样。
自编码器 → 生成模型？
我们会有一个很自然的做生成模型的想法，那就是训练一个从低维隐层变量生成观测样本的生成模型，最大化观测数据似然。
假设这个生成模型为 $G_{\theta}:\mathbb{R}^k \rightarrow \mathbb{R}^d$，其中 $k &amp;lt; d$，将隐层变量 $Z$ 映射为样本 $X$，那么其实在样本空间里几乎大部分区域 $p(X)=0$。
如果我们从样本空间看，在这个高维空间只会有非常小的一个低维空间子集 $p(X)$ 是有值的，并且我们在训练的时候其实是不知道这个子集的分布的，而其余大部分区域 $p(X)=0$，也就意味着我们很难直接优化似然。
但是有一种方法可以让我们在每一处都得到非零值，那就是在已有先验 $p(Z)$ 的条件下，定义一个有噪声的观测模型 $p_\theta(X|Z)=\mathcal{N}(X;G_\theta(Z), \eta I)$ (其中 $\eta$ 是可调整的参数，$I$ 是单位矩阵)。
所以 $p(X) = \int p(Z)p(X|Z)\mathrm{d}Z$，这个值也很难去计算，所以我们不是去优化 $p(X)$ 而是去优化 $p(X)$ 的下限（变分推断里的证据下限 ELBO，后面会证明）。
那么这其实就是 VAE 的雏形了。
变分自编码器 VAE 上面的图是 VAE 的整体思路，生成的部分也是也就是 decoder 的部分，我们会假设 $Z$ 服从一个简单的先验分布 $p(Z)$，这个分布可以是一个标准正态分布。通过 decoder 会得到高维图像空间的一个概率分布。</description>
    </item>
    
    <item>
      <title>端到端图像/视频压缩里的熵模型</title>
      <link>https://imfaye.me/post/e2e-entropy-model/</link>
      <pubDate>Wed, 20 Apr 2022 17:04:37 +0800</pubDate>
      
      <guid>https://imfaye.me/post/e2e-entropy-model/</guid>
      <description>概率分布与熵编码 在端到端图像/视频压缩模型中，我们需要去尽可能精准地模拟待编码元素值的概率分布。一方面是为了更精确地进行码率估计，另一方面也是因为更精准的概率分布建模能使得熵编码环节更好地消除统计冗余节省码字。
建模出概率分布后，在实际熵编码中，就可以通过概率分布生成熵编码器所需要的概率表。
多说一句，在传统编解码里，通常熵编码会采用自适应模型，即随着编码字符的输入，不断更新概率分布（自适应模型相比静态模型效率更高，符合局部性原理，适应符号概率忽大忽小的波动，如果能合理地利用上下文信息压缩效率可以远超静态模型）。然而在端到端压缩模型里，通常直接通过网络生成独立的概率分布的参数，不会随着编码过程更新概率表。
量化不可导 这个没什么好说的，量化四舍五入的取整操作显然是不可导的，所以在训练的时候可以通过加均匀噪声来替换四舍五入的操作。
在训练阶段，我们会通过给待编码元素值加上-0.5到0.5的均匀噪声来替代量化操作。而实际推理的时候，就正常进行量化。
均匀噪声涉及到的概率关系 首先明确一下这几条线分布代表什么。
$p_{y_i}$：编码空间元素值的概率密度函数
$p_{\tilde{y_i}}$：$y_i$ 加上均匀噪声后的概率密度函数
$p_{\hat{y_i}}$：$y_i$量化后的概率质量函数（量化后就成了离散型变量了）
均匀噪声其实就是均匀分布 $U(-0.5, 0.5)$，$y_i$ 加上均匀噪声得到 $\tilde{y_i}$，两个独立的连续随机变量的和的概率分布公式是 $f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x) ,\mathrm{d}x$，直观来说也很好理解，对于任意 $\tilde{y_i}$ 值为 $c$，可能加均匀噪声得到 $c$ 的 $y_i$ 取值范围其实就是 $c-0.5$ 到 $c+0.5$，$p_{\tilde{y_i}}$ 在 $c$ 点的值其实就可以通过 $p_{y_i}$ 在 $c-0.5$ 到 $c+0.5$ 的积分得出。
对于每个整数点，也自然符合上述描述。
而这样一来，加均匀噪声得到的 $p_{\tilde{y_i}}$ 最妙的性质就在于，在每个整数点 $p_{\tilde{y_i}}$ 的值和实际量化得到的离散变量 $p_{\hat{y_i}}$ 在这一点的概率质量相等。
所以说，加均匀噪声这一操作，本质上类似于在给 $p_{\hat{y_i}}$ 的概率质量函数作插值，类似于一个连续松弛 (continuous relaxation) 的操作。
此外，我们在端到端模型里通常去建模的也就是这个 $p_{y_i}$，而这里其实是假设 $p_{y_i}$ 近似一个拉普拉斯分布，实际代码实现中，有一部分模型采用拉普拉斯分布去建模，也有一部分模型，比如 CompressAI，是采用高斯分布去建模的。
CompressAI 代码中的熵模型 以其中的 GaussianConditional 熵模型为例，稍微讲一下实际实现的时候一些常见操作。
def forward( self, inputs: Tensor, scales: Tensor, means: Optional[Tensor] = None, training: Optional[bool] = None, ) -&amp;gt; Tuple[Tensor, Tensor]: if training is None: training = self.</description>
    </item>
    
    <item>
      <title>可变形卷积与光流</title>
      <link>https://imfaye.me/post/dcn-and-optical-flow/</link>
      <pubDate>Sun, 20 Feb 2022 17:04:37 +0800</pubDate>
      
      <guid>https://imfaye.me/post/dcn-and-optical-flow/</guid>
      <description>可变形卷积代码篇 一个调用 from mmcv.ops import ModulatedDeformConv2d 的例子：
def forward(self, input): x = self.offset_mask_conv(input) o1, o2, mask = torch.chunk(x, 3, dim=1) offset = torch.cat((o1, o2), dim=1) mask = torch.sigmoid(mask) output = self.dcnv2(input, offset, mask) return output 总而言之 offset 的 size 就是 2*kernel[0]*kernel[1] ，想一想，原来我们求偏移的时候，会把 B*H*W*C 的图像送入普通卷积得到 B*H*W*2C 得到偏移，也就是每个通道每个位置点都有 x 和 y 两个方向的偏移量。
对 DCN 来说，每个通道都做一样的处理，也就是只需要对每个位置点存卷积核每个点的 x 和 y 的偏移，所以就是 B*H*W*(2*kernel_size) 。
关于 mask:：置信 mask，并非必需，不作展开了
所以 offset 和 mask 一起就是 B*H*W*(3*kernel_size)
关于 deform_groups：本来是所有通道公用，也可以改成划成几组，组数就是 deform_groups，那这样就是 B*H*W*(group_num*3*kernel_size)</description>
    </item>
    
  </channel>
</rss>
