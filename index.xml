<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>PIKA☆NCHI</title>
    <link>https://imfaye.me/</link>
    <description>Recent content on PIKA☆NCHI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Faye</copyright>
    <lastBuildDate>Sun, 15 Jan 2023 15:04:21 +0800</lastBuildDate>
    
        <atom:link href="https://imfaye.me/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>我所喜欢的河濑直美</title>
        <link>https://imfaye.me/post/kawase-naomi/</link>
        <pubDate>Sun, 15 Jan 2023 15:04:21 +0800</pubDate>
        
        <guid>https://imfaye.me/post/kawase-naomi/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/kawase-naomi/ -&lt;h2 id=&#34;最近偏爱河濑直美&#34;&gt;最近偏爱河濑直美&lt;/h2&gt;
&lt;p&gt;很久之前就想看萌之朱雀，那段时间很喜欢超8mm摄像机的影片，目测剧情觉得相当工整并没第一时间看。前段时间偶然扫过沙罗双树的剧情简介，古都，双生子，神隐，祭典，这些词的意象以及剧照都相当吸引我。看完之后就开始沉迷河濑直美。&lt;/p&gt;
&lt;p&gt;河濑直美确实是“一生只拍一部电影”的典型，始终徘徊在身世问题左右。想起她在东大入学仪式的致辞，也说了一直一直只盯着一扇窗，这一扇窗和世界的联结又也能让她和世界上众多的人相遇。身世，原生家庭，童年，似乎很多人长久执着于这些母题，有的时候我在想，是不是就算长大以后，我们作为人类的理解力也就只能帮助我们消化自我到青春期以前的这个节点。&lt;/p&gt;
&lt;p&gt;我无限偏爱她抒情化的空镜头，泛出的柔光、环境音、易碎的颤动感。她镜头下的静物，和静止的相片相比，动人的正是时间穿行而过的窸窣感，柔光下的树木枝叶，风摇动的粼粼的光与阴影，触碰窗沿结着的雨珠，水的滴落，碗中浸泡的青豆浮动的光泽，晾晒的衣物被吹起的摆动，空转的圆盘形衣架。有某种薄如蝉翼的果冻一般的轻颤。&lt;/p&gt;
&lt;p&gt;被凝视着的安静的时间流淌，影像中的时间流速和现实有了同步的校正。就好像沙罗双树里俊骑单车载着夕现实里很短影像中又显得很长的那一段路。&lt;/p&gt;
&lt;p&gt;看了早期的短片，拥抱、在世界的沉默中、蜗牛、见到天堂，觉得自己可以一直一直看下去河濑直美的短片，一直一直平静下去，一直一直落入回忆的质感。电影里最喜欢的果然还是萌之朱雀和沙罗双树。也看了澄沙之味和光，故事却远远没有散景好。&lt;/p&gt;
&lt;h2 id=&#34;沙罗双树&#34;&gt;沙罗双树&lt;/h2&gt;
&lt;p&gt;喜欢这一段长长的骑行路。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776276.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776319.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776472.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776538.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776606.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;沙罗节，暴雨。日光和暴雨下的舞、喊词、两旁人群的加入、压抑许久释放出的笑。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776613.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;蜗牛&#34;&gt;蜗牛&lt;/h2&gt;
&lt;p&gt;代表理性旁观的摄影机镜头，代表个人情感的伸手触碰。&lt;/p&gt;
&lt;p&gt;印象深刻的是，摄像机一直正面面向祖母，靠近靠近又靠近，祖母笑了，说干嘛一直拍我，怎么不拍拍你自己。祖母笑着说，不要拍啦。祖母笑着说，我又不是很快就要死掉。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776964.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776992.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777013.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777021.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777111.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777139.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;拥抱&#34;&gt;拥抱&lt;/h2&gt;
&lt;p&gt;我很喜欢日光过曝又暗下去的反复，像长时间安静站立在原地阳光被一片一片云遮挡又穿透而出，像记忆里背景光的质感。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777175.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777211.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777195.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777201.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777221.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777231.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777285.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;在世界的沉默中&#34;&gt;在世界的沉默中&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777303.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777360.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777310.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777364.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
- https://imfaye.me/post/kawase-naomi/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>耶，VAE</title>
        <link>https://imfaye.me/post/vae/</link>
        <pubDate>Sun, 10 Jul 2022 23:43:59 +0000</pubDate>
        
        <guid>https://imfaye.me/post/vae/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/vae/ -&lt;h1 id=&#34;引子&#34;&gt;引子&lt;/h1&gt;
&lt;h2 id=&#34;生成模型&#34;&gt;生成模型&lt;/h2&gt;
&lt;p&gt;能从可学习的概率分布中采样得到样本的模型。&lt;/p&gt;
&lt;p&gt;在一些生成模型中，样本通过将随机的隐层变量送入网络生成得到。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234631635.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;自编码器-ae&#34;&gt;自编码器 AE&lt;/h2&gt;
&lt;p&gt;自编码器通过学习从输入到隐层和从隐层到输出的映射来重建信号/图像。&lt;/p&gt;
&lt;p&gt;目标：$X&amp;rsquo; = D_\theta(E_\phi(X)) \approx X$&lt;/p&gt;
&lt;p&gt;$\mathop{min}\limits_{\theta, \phi} \sum\limits_{i=1}^n||D_\theta(E_\phi(X_i))-X_i||^2$，其中 ${{X_i}}_{i=1\cdots n}$ 为数据集。&lt;/p&gt;
&lt;p&gt;自编码器并不是一种生成模型，因为它并没有定义一个概率分布，无法采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自编码器 → 生成模型？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们会有一个很自然的做生成模型的想法，那就是训练一个从低维隐层变量生成观测样本的生成模型，最大化观测数据似然。&lt;/p&gt;
&lt;p&gt;假设这个生成模型为 $G_{\theta}:\mathbb{R}^k \rightarrow \mathbb{R}^d$，其中 $k &amp;lt; d$，将隐层变量 $Z$ 映射为样本 $X$，那么其实在样本空间里几乎大部分区域 $p(X)=0$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234646078.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果我们从样本空间看，在这个高维空间只会有非常小的一个低维空间子集 $p(X)$ 是有值的，并且我们在训练的时候其实是不知道这个子集的分布的，而其余大部分区域 $p(X)=0$，也就意味着我们很难直接优化似然。&lt;/p&gt;
&lt;p&gt;但是有一种方法可以让我们在每一处都得到非零值，那就是在已有先验 $p(Z)$ 的条件下，定义一个有噪声的观测模型 $p_\theta(X|Z)=\mathcal{N}(X;G_\theta(Z), \eta I)$ (其中 $\eta$ 是可调整的参数，$I$ 是单位矩阵)。&lt;/p&gt;
&lt;p&gt;所以 $p(X) = \int p(Z)p(X|Z)\mathrm{d}Z$，这个值也很难去计算，所以我们不是去优化 $p(X)$ 而是去优化 $p(X)$ 的下限（变分推断里的证据下限 ELBO，后面会证明）。&lt;/p&gt;
&lt;p&gt;那么这其实就是 VAE 的雏形了。&lt;/p&gt;
&lt;h1 id=&#34;变分自编码器-vae&#34;&gt;变分自编码器 VAE&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234659598.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;上面的图是 VAE 的整体思路，生成的部分也是也就是 decoder 的部分，我们会假设 $Z$ 服从一个简单的先验分布 $p(Z)$，这个分布可以是一个标准正态分布。通过 decoder 会得到高维图像空间的&lt;strong&gt;一个概率分布&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而 encoder 端，注意 VAE 有一个很重要的想法是，我们不去直接计算难以计算的 $p_\theta(Z|X)$，而是用另外单独学习的网络去模拟一个 $q_\phi(Z|X)$，它其实是 $p_\theta(Z|X)$ 的一个近似。&lt;/p&gt;
&lt;h2 id=&#34;p_thetax-推导&#34;&gt;$p_\theta(X)$ 推导&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234713512.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再看一下 $p_\theta(X)$  下界的推导过程。①：因为 $p_\theta(X)$ 是独立于 $Z$ 的，所以我们可以去计算它在 $Z$ 上的期望；②：条件概率公式；③：上下约一个 $q_\phi(Z|X)$；④：右边的项其实就是 $q_\phi(Z|X)$ 和 $p_\theta(Z|X)$ 的 KL 散度，KL 散度衡量的是两个概率分布之间的相似性，两者差异越小，KL 散度越小，两分布完全一致时 KL 散度才为 0，所以因为右项恒大于等于 0，我们可以把左项视为 $log\space p_\theta(X)$ 的下界。之后就不直接优化 $log\space p_\theta(X)$，而是优化这个下界。&lt;/p&gt;
&lt;h2 id=&#34;variational-lower-bound-的直观解释&#34;&gt;Variational Lower Bound 的直观解释&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234726362.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再来看一下怎么理解这个变分下界，注意我们的目标是最大化这个下界。首先用条件概率公式替换一下，之后把式子拆成两部分，下面来解释一下为什么第一项是重建误差，第二项是正则项。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234737121.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;之前说了我们建立了一个有噪声的观测模型 $p_\theta(X|Z)=\mathcal{N}(X;G_\theta(Z), \eta I)$ (就是 $Z$ 通过 decoder 后得到的那个高维图像空间的分布)，正态分布的公式不用说了吧，代入一下就会发现第一项是一个 L2 距离，最大化这个负的 L2 距离就是在减小重建误差，encourage $q_\phi$ to be point mass，这句话我是这么理解的，point mass 其实是离散的概率分布，减小重建误差就是在消除我们加的这个高斯噪声，让它成为类似于我们最先讲的那个被舍弃的点到点的离散概率模型（但注意它又是连续概率，所以就是接近奇异分布？）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234745973.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再看第二项，这一项可以写成一个 KL 散度，我们最大化负的这个 KL 散度就是在让 $q_\phi(Z|X)$ 和 $p(Z)$ 两个概率分布尽可能接近。上一项重建损失是鼓励 $q_\phi$ 去成为 point mass，这里则是平滑 $q_\phi$ 去使它尽可能接近标准正态分布。&lt;/p&gt;
&lt;p&gt;可以看到两项之间存在相驳的张力，前一项试图让 $q_\phi$ 成为奇异分布，后一项则试图让 $q_\phi$ 不要成为奇异分布。可以理解为前者鼓励它准，后者鼓励它具有更强的生成性。&lt;/p&gt;
&lt;h2 id=&#34;vae-架构&#34;&gt;VAE 架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234755246.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再来看一下 VAE 的实际架构。主要有两点值得细说。&lt;/p&gt;
&lt;p&gt;第一点是，如果我们对每个 $X_i$ 找最最佳的 $q_\phi(Z|X_i)$，然后优化 $\phi$，这样的更新代价会很大。所以我们不这么做，而是去学习一个 inference 网络预测这个 $q_\phi(Z|X_i)$ 的均值和方差（实际上预测的是  $\mu$ 和 $log \space \sigma$），这样 inference 阶段的模型参数对于所有的数据参数是共享的，就可以分摊学习和更新的成本。&lt;/p&gt;
&lt;p&gt;第二点是，采样的操作本身是不能反向传播的，所以采样这里用到了重参数化的技巧，也就是从 $\mathcal{N}(0,1)$ 中采样一个 $\varepsilon$，然后让 $Z=\mu + \varepsilon \times \sigma$，这样采样的操作就可以独立于网络之外，其他所有环节都能进行反向传播。&lt;/p&gt;
&lt;h2 id=&#34;stochastic-gradient-optimization-of-vlb&#34;&gt;Stochastic Gradient Optimization of VLB&lt;/h2&gt;
&lt;p&gt;Todo…&lt;/p&gt;
&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://khoury.northeastern.edu/home/hand/teaching/cs7150-summer-2020/Variational_Autoencoders.pdf&#34;&gt;https://khoury.northeastern.edu/home/hand/teaching/cs7150-summer-2020/Variational_Autoencoders.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=c27SHdQr4lw&#34;&gt;https://www.youtube.com/watch?v=c27SHdQr4lw&lt;/a&gt; （力荐 👍）&lt;/p&gt;
- https://imfaye.me/post/vae/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>端到端图像/视频压缩里的熵模型</title>
        <link>https://imfaye.me/post/e2e-entropy-model/</link>
        <pubDate>Wed, 20 Apr 2022 17:04:37 +0800</pubDate>
        
        <guid>https://imfaye.me/post/e2e-entropy-model/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/e2e-entropy-model/ -&lt;h2 id=&#34;概率分布与熵编码&#34;&gt;概率分布与熵编码&lt;/h2&gt;
&lt;p&gt;在端到端图像/视频压缩模型中，我们需要去尽可能精准地模拟待编码元素值的概率分布。一方面是为了更精确地进行码率估计，另一方面也是因为更精准的概率分布建模能使得熵编码环节更好地消除统计冗余节省码字。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230224_1677253790.png&#34; alt=&#34;image-20230224234946517&#34;&gt;&lt;/p&gt;
&lt;p&gt;建模出概率分布后，在实际熵编码中，就可以通过概率分布生成熵编码器所需要的概率表。&lt;/p&gt;
&lt;p&gt;多说一句，在传统编解码里，通常熵编码会采用自适应模型，即随着编码字符的输入，不断更新概率分布（自适应模型相比静态模型效率更高，符合局部性原理，适应符号概率忽大忽小的波动，如果能合理地利用上下文信息压缩效率可以远超静态模型）。然而在端到端压缩模型里，通常直接通过网络生成独立的概率分布的参数，不会随着编码过程更新概率表。&lt;/p&gt;
&lt;h2 id=&#34;量化不可导&#34;&gt;量化不可导&lt;/h2&gt;
&lt;p&gt;这个没什么好说的，量化四舍五入的取整操作显然是不可导的，所以在训练的时候可以通过加均匀噪声来替换四舍五入的操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677299134.png&#34; alt=&#34;image-20230225122531069&#34;&gt;&lt;/p&gt;
&lt;p&gt;在训练阶段，我们会通过给待编码元素值加上-0.5到0.5的均匀噪声来替代量化操作。而实际推理的时候，就正常进行量化。&lt;/p&gt;
&lt;h2 id=&#34;均匀噪声涉及到的概率关系&#34;&gt;均匀噪声涉及到的概率关系&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677304856.png&#34; alt=&#34;image-20230225124013214&#34;&gt;&lt;/p&gt;
&lt;p&gt;首先明确一下这几条线分布代表什么。&lt;/p&gt;
&lt;p&gt;$p_{y_i}$：编码空间元素值的概率密度函数&lt;/p&gt;
&lt;p&gt;$p_{\tilde{y_i}}$：$y_i$ 加上均匀噪声后的概率密度函数&lt;/p&gt;
&lt;p&gt;$p_{\hat{y_i}}$：$y_i$量化后的概率质量函数（量化后就成了离散型变量了）&lt;/p&gt;
&lt;p&gt;均匀噪声其实就是均匀分布 $U(-0.5, 0.5)$，$y_i$ 加上均匀噪声得到 $\tilde{y_i}$，两个独立的连续随机变量的和的概率分布公式是 $f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x) ,\mathrm{d}x$，直观来说也很好理解，对于任意 $\tilde{y_i}$ 值为 $c$，可能加均匀噪声得到 $c$ 的 $y_i$ 取值范围其实就是 $c-0.5$ 到 $c+0.5$，$p_{\tilde{y_i}}$ 在 $c$ 点的值其实就可以通过 $p_{y_i}$ 在 $c-0.5$ 到 $c+0.5$ 的积分得出。&lt;/p&gt;
&lt;p&gt;对于每个整数点，也自然符合上述描述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677304320.png&#34; alt=&#34;image-20230225135158633&#34;&gt;&lt;/p&gt;
&lt;p&gt;而这样一来，加均匀噪声得到的 $p_{\tilde{y_i}}$ 最妙的性质就在于，在每个整数点 $p_{\tilde{y_i}}$ 的值和实际量化得到的离散变量 $p_{\hat{y_i}}$ 在这一点的概率质量相等。&lt;/p&gt;
&lt;p&gt;所以说，加均匀噪声这一操作，本质上类似于在给 $p_{\hat{y_i}}$ 的概率质量函数作插值，类似于一个连续松弛 (continuous relaxation) 的操作。&lt;/p&gt;
&lt;p&gt;此外，我们在端到端模型里通常去建模的也就是这个 $p_{y_i}$，而这里其实是假设 $p_{y_i}$ 近似一个拉普拉斯分布，实际代码实现中，有一部分模型采用拉普拉斯分布去建模，也有一部分模型，比如 CompressAI，是采用高斯分布去建模的。&lt;/p&gt;
&lt;h2 id=&#34;compressai-代码中的熵模型&#34;&gt;CompressAI 代码中的熵模型&lt;/h2&gt;
&lt;p&gt;以其中的 GaussianConditional 熵模型为例，稍微讲一下实际实现的时候一些常见操作。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;forward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inputs: Tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        scales: Tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        means: Optional[Tensor] = &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        training: Optional[&lt;span style=&#34;color:#658b00&#34;&gt;bool&lt;/span&gt;] = &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ) -&amp;gt; Tuple[Tensor, Tensor]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; training &lt;span style=&#34;color:#8b008b&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            training = self.training
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        outputs = self.quantize(inputs, &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;noise&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; training &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;dequantize&amp;#34;&lt;/span&gt;, means)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        likelihood = self._likelihood(outputs, scales, means)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; self.use_likelihood_bound:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            likelihood = self.likelihood_lower_bound(likelihood)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; outputs, likelihood
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;训练的时候和上面说的一样，通过加均匀噪声替代量化操作。&lt;/p&gt;
&lt;p&gt;而其中这个 &lt;code&gt;likelihood&lt;/code&gt; 其实就是用于码率估计的，网络会输出 $y_i$ 值，然后我们叠加均匀噪声，而网络也会输出 $p_{y_i}$ 建模为高斯分布的 $\mu$ 和 $\sigma$ 值，这样其实我们就能计算出当前条件下 $p_{\tilde{y_i}}$ 的特定取值。后面再用 $-log_2 x$ 就能算出估计出的码字比特大小。&lt;/p&gt;
&lt;p&gt;再来看一下实际熵编解码的实现，以编码为例。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;compress&lt;/span&gt;(self, inputs, indexes, means=&lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;        Compress input tensors to char strings.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;        Args:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;            inputs (torch.Tensor): input tensors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;            indexes (torch.IntTensor): tensors CDF indexes
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;            means (torch.Tensor, optional): optional tensor means
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        symbols = self.quantize(inputs, &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;symbols&amp;#34;&lt;/span&gt;, means)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#658b00&#34;&gt;len&lt;/span&gt;(inputs.size()) &amp;lt; &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#008b45;font-weight:bold&#34;&gt;ValueError&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;Invalid `inputs` size. Expected a tensor with at least 2 dimensions.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; inputs.size() != indexes.size():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#008b45;font-weight:bold&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;`inputs` and `indexes` should have the same size.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._check_cdf_size()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._check_cdf_length()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._check_offsets_size()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        strings = []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#8b008b&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#658b00&#34;&gt;range&lt;/span&gt;(symbols.size(&lt;span style=&#34;color:#b452cd&#34;&gt;0&lt;/span&gt;)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            rv = self.entropy_coder.encode_with_indexes(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                symbols[i].reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                indexes[i].reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self._quantized_cdf.tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self._cdf_length.reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self._offset.reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            strings.append(rv)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; strings
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;symbols&lt;/code&gt; 就是量化后的待编码值减去网络预测出的高斯分布的均值，这样后面熵编码就可以统一用准备好的不同方差的零均值高斯采样的 cdf 表。再注意一下这里的 &lt;code&gt;indexes&lt;/code&gt; ，下文会讲。&lt;/p&gt;
&lt;p&gt;上面说的 cdf 表可以通过这个更新函数看一下实现过程。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;update&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        multiplier = -self._standardized_quantile(self.tail_mass / &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pmf_center = torch.ceil(self.scale_table * multiplier).int()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pmf_length = &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt; * pmf_center + &lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_length = torch.max(pmf_length).item()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        device = pmf_center.device
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples = torch.abs(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            torch.arange(max_length, device=device).int() - pmf_center[:, &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples_scale = self.scale_table.unsqueeze(&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples = samples.float()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples_scale = samples_scale.float()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        upper = self._standardized_cumulative((&lt;span style=&#34;color:#b452cd&#34;&gt;0.5&lt;/span&gt; - samples) / samples_scale)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        lower = self._standardized_cumulative((-&lt;span style=&#34;color:#b452cd&#34;&gt;0.5&lt;/span&gt; - samples) / samples_scale)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pmf = upper - lower
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tail_mass = &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt; * lower[:, :&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        quantized_cdf = torch.Tensor(&lt;span style=&#34;color:#658b00&#34;&gt;len&lt;/span&gt;(pmf_length), max_length + &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        quantized_cdf = self._pmf_to_cdf(pmf, tail_mass, pmf_length, max_length)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._quantized_cdf = quantized_cdf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._offset = -pmf_center
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._cdf_length = pmf_length + &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到，cdf 表其实是一定数量采样的 scale 值对应的零均值高斯分布（转化后的 $p_{y_i}$）在一定数量采样点上计算好 $p_{\tilde{y_i}}$ 的值。&lt;/p&gt;
&lt;p&gt;这里其实有两处采样：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;采样 scale （对应 index ，index 指明取 cdf 表哪个分布）&lt;/li&gt;
&lt;li&gt;对于分布采样一系列的点计算概率值 （对应 cdf 表中每个分布）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可以通过 &lt;code&gt;build_indexes&lt;/code&gt; 来看一下 indexes 的确定过程。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;build_indexes&lt;/span&gt;(self, scales: Tensor) -&amp;gt; Tensor:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        scales = self.lower_bound_scale(scales)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        indexes = scales.new_full(scales.size(), &lt;span style=&#34;color:#658b00&#34;&gt;len&lt;/span&gt;(self.scale_table) - &lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;for&lt;/span&gt; s &lt;span style=&#34;color:#8b008b&#34;&gt;in&lt;/span&gt; self.scale_table[:-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            indexes -= (scales &amp;lt;= s).int()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; indexes
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;indexes&lt;/code&gt; 是通过 &lt;code&gt;scales&lt;/code&gt; 来确定的，具体来说 &lt;code&gt;index&lt;/code&gt; 其实是之前说的一系列采样的 scale 值里小于等于当前 scale 的最接近它的采样 scale 的编号。&lt;/p&gt;
- https://imfaye.me/post/e2e-entropy-model/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>可变形卷积与光流</title>
        <link>https://imfaye.me/post/dcn-and-optical-flow/</link>
        <pubDate>Sun, 20 Feb 2022 17:04:37 +0800</pubDate>
        
        <guid>https://imfaye.me/post/dcn-and-optical-flow/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/dcn-and-optical-flow/ -&lt;h2 id=&#34;可变形卷积代码篇&#34;&gt;可变形卷积代码篇&lt;/h2&gt;
&lt;p&gt;一个调用 &lt;code&gt;from mmcv.ops import ModulatedDeformConv2d&lt;/code&gt; 的例子：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;forward&lt;/span&gt;(self, &lt;span style=&#34;color:#658b00&#34;&gt;input&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x = self.offset_mask_conv(&lt;span style=&#34;color:#658b00&#34;&gt;input&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        o1, o2, mask = torch.chunk(x, &lt;span style=&#34;color:#b452cd&#34;&gt;3&lt;/span&gt;, dim=&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        offset = torch.cat((o1, o2), dim=&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        mask = torch.sigmoid(mask)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output = self.dcnv2(&lt;span style=&#34;color:#658b00&#34;&gt;input&lt;/span&gt;, offset, mask)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; output
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;总而言之 &lt;code&gt;offset&lt;/code&gt; 的 size 就是 &lt;code&gt;2*kernel[0]*kernel[1]&lt;/code&gt; ，想一想，原来我们求偏移的时候，会把 &lt;code&gt;B*H*W*C&lt;/code&gt; 的图像送入普通卷积得到 &lt;code&gt;B*H*W*2C&lt;/code&gt; 得到偏移，也就是每个通道每个位置点都有 x 和 y 两个方向的偏移量。&lt;/p&gt;
&lt;p&gt;对 DCN 来说，每个通道都做一样的处理，也就是只需要对每个位置点存卷积核每个点的 x 和 y 的偏移，所以就是 &lt;code&gt;B*H*W*(2*kernel_size)&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;关于 mask:：置信 mask，并非必需，不作展开了&lt;/p&gt;
&lt;p&gt;所以 offset 和 mask 一起就是 &lt;code&gt;B*H*W*(3*kernel_size)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;关于 deform_groups：本来是所有通道公用，也可以改成划成几组，组数就是 deform_groups，那这样就是 &lt;code&gt;B*H*W*(group_num*3*kernel_size)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;可变形卷积与光流&#34;&gt;可变形卷积与光流&lt;/h2&gt;
&lt;p&gt;端到端视频压缩模型里，运动估计运动补偿环节，常用到光流和可变形卷积。本质上都是预测偏移，只不过 DCN 可以利用多个偏移，这样在预测难度较大的位置就有多个 offset 互作补充，所以比只用单一的光流更有优势一些。&lt;/p&gt;
&lt;p&gt;推荐一下这篇文章：&lt;a href=&#34;https://ckkelvinchan.github.io/projects/DCN/&#34;&gt;Understanding Deformable Alignment in Video Super-Resolution&lt;/a&gt;，讲得比较透彻了。&lt;/p&gt;
&lt;h3 id=&#34;可变形卷积涉及多少个不同的-offset-map&#34;&gt;可变形卷积涉及多少个不同的 offset map&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先，与 kernel size 有关&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677309897.png&#34; alt=&#34;image-20230225152456053&#34;&gt;&lt;/p&gt;
&lt;p&gt;假设特征图大小为 &lt;code&gt;W * H&lt;/code&gt;，每个点的感受野都对应了 &lt;code&gt;kernel_size * kernel_size&lt;/code&gt; 个偏移&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其次，与 group num 有关&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677310198.png&#34; alt=&#34;image-20230225152648179&#34;&gt;&lt;/p&gt;
&lt;p&gt;每 &lt;code&gt;C(通道数) / group_num&lt;/code&gt; 个通道共用一套 offset&lt;/p&gt;
&lt;p&gt;比如假设特征有 8 个通道，group_num 为 4,则 每 2 个通道共用一套偏移参数&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;综合来说，一共会有 &lt;code&gt;(kernel_size * kernel_size) * group_num&lt;/code&gt; 个 &lt;code&gt;W * H&lt;/code&gt; 大小的偏移图。&lt;/p&gt;
&lt;h3 id=&#34;dcn-对齐和光流对齐的本质差异&#34;&gt;DCN 对齐和光流对齐的本质差异&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677310203.png&#34; alt=&#34;image-20230225152953943&#34;&gt;&lt;/p&gt;
&lt;p&gt;$kernel\space size 为 n&lt;em&gt;n的DCN = n^2个空间warping \space +\space 1&lt;/em&gt;1*n^2的3D卷积$&lt;/p&gt;
&lt;p&gt;也就是说，如果 n=1, group_num=1，DCN对齐基本就等同于光流对齐，DCN 和 光流的本质区别就在于 offset diversity。&lt;/p&gt;
- https://imfaye.me/post/dcn-and-optical-flow/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>一些传统的熵编码方法</title>
        <link>https://imfaye.me/post/entropy-coding/</link>
        <pubDate>Sun, 05 Dec 2021 13:09:24 +0000</pubDate>
        
        <guid>https://imfaye.me/post/entropy-coding/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/entropy-coding/ -&lt;h1 id=&#34;传统熵模型&#34;&gt;传统熵模型&lt;/h1&gt;
&lt;h2 id=&#34;算术编码-arithmetic-coding&#34;&gt;算术编码 (Arithmetic Coding)&lt;/h2&gt;
&lt;h3 id=&#34;流程&#34;&gt;流程&lt;/h3&gt;
&lt;p&gt;（以静态模型举例）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假设有一段数据需要编码，统计里面所有的字符和出现的次数。编码从初始区间 (0, 1] 开始。&lt;/li&gt;
&lt;li&gt;在当前区间内根据各字符概率划分子区间。&lt;/li&gt;
&lt;li&gt;读入字符，找到该字符落入的子区间，将区间更新为该子区间，并重复 2, 3 步骤&lt;/li&gt;
&lt;li&gt;最后得到的区间 [low, high) 中任意一个小数以二进制形式输出即得到编码的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例子如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727131100497.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;实现细节&#34;&gt;实现细节&lt;/h3&gt;
&lt;p&gt;最后结果是一个小数，我们不能简单地用一个 double 类型去表示和计算这个小数，因为根据数据的复杂程度，这个小数可能任意长，小数点后可能会有成千上万位。&lt;/p&gt;
&lt;p&gt;然而，小数点后的数据前几位很有可能是在过程中是可以不断提前确定的。例如如果当前区间为 [0.14432, 0.1456)，高位的 0.14 可以提前确定，14已经可以输出了。那么小数点可以向后移动两位，区间变成 [0.432, 0.56)，在此基础上进行后面的计算。这样编码区间永远保持在一个有限的精度要求上。&lt;/p&gt;
&lt;p&gt;上述是基于十进制的，实际数字是用二进制表示的，当然原理是一样的，用十进制只是为了表述方便。&lt;/p&gt;
&lt;h3 id=&#34;静态模型--自适应模型&#34;&gt;静态模型 → 自适应模型&lt;/h3&gt;
&lt;p&gt;静态模型在初始时对完整的数据统计完概率分布，之后不再更新概率分布；自适应模型随着字符的输入会不断更新概率分布。&lt;/p&gt;
&lt;p&gt;静态模型的缺点在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在压缩前对信息内字符进行统计的过程会消耗大量时间。&lt;/li&gt;
&lt;li&gt;对较长的信息，静态模型统计出的符号概率是该符号在整个信息中的出现概率，而自适应模型可以统计出某个符号在某一局部的出现概率或某个符号相对于某一上下文的出现概率，换句话说，自适应模型得到的概率分布将有利于对信息的压缩（可以说结合上下文的自适应模型的信息熵建立在更高的概率层次上，其总熵值更小），好的基于上下文的自适应模型得到的压缩结果将远远超过静态模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如一段码流，某符号在前面出现概率较大而后面概率小，甚至忽大忽小，采用自适应模型就可以更好的适应这样的变动，压缩效率会比静态模型更高。主流视频编码标准如H.264/H.265都使用自适应模型。&lt;/p&gt;
&lt;h3 id=&#34;算术编码-vs-哈夫曼编码&#34;&gt;算术编码 vs 哈夫曼编码&lt;/h3&gt;
&lt;p&gt;首先说结论，算术编码压缩效率更高，哈夫曼编码复杂度更低。&lt;/p&gt;
&lt;p&gt;这两种编码，或者说熵编码的本质是，概率越小的字符，用更多的 bit 去表示，这反映到概率区间上就是，概率小的字符所对应的区间也小，因此这个区间的上下边际值的差值越小，为了唯一确定当前这个区间，则需要更多的数字去表示它。&lt;/p&gt;
&lt;p&gt;哈夫曼编码由于不断地二叉，它的子区间总是 $\frac{1}{2}$ 的幂次方。而算术编码可以做到严格按照概率的大小等比例划分子区间。所以哈夫曼编码只是算术编码一种粗略的近似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727131117834.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;cabac&#34;&gt;CABAC&lt;/h3&gt;
&lt;p&gt;CABAC（Context-based Adaptive Binary Arithmetic Coding），CABAC 被视频标准H.264/H.265所采用。&lt;/p&gt;
&lt;p&gt;CABAC可以分为二值化、上下文建模和二进制算术编码三个步骤。&lt;/p&gt;
&lt;p&gt;其中上下文建模相当于把整段码流进行了再次的细分，把相同条件下的字符bin（比如块大小/亮度色度/语法元素/扫描方式/周围情况等）归属于某个context，形成一个比较独立的子队列而进行编码，其更新只与当前的状态和当前字符是否MPS有关（换句话说，只和历史该子队列编码字符和当前字符有关），而与别的子队列/字符是无关的。当然输出码字往往是根据规则而“混”在一起的。&lt;/p&gt;
&lt;p&gt;CABAC虽然性能很好，但也存在以下几点不足：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;复杂度过高，不易并行处理。存在块级依赖（左/上角的块没有码率估计/熵编码，后继块就无法得到更新后的状态，从而无法开始码率估计/熵编码）、Bin级依赖（同一个子队列的bin存在前后依赖性，后继的bin要等前面bin编完后才能得到更新后的上下文状态）以及编码的几个环节依赖，这些依赖性会影响编码器的并行实现。&lt;/li&gt;
&lt;li&gt;计算精度问题。为简化计算，CABAC采用128个状态来近似，根据原来状态和当前符号性质查表得到下个状态。这个过程中会有一些精度的损失。另外，如果当一连串的MPS到来，状态到达62后就不会继续改变，只会“原地踏步”。换句话说，当概率到达0.01975时就不会随着符号继续变小，这样会影响压缩效率。&lt;/li&gt;
&lt;li&gt;Context的设计问题。部分context利用频率很低，在测试中平均一帧都用不到几次，而有的context使用频率很高，需要进一步的优化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;区间编码-range-coding&#34;&gt;区间编码 (Range Coding)&lt;/h2&gt;
&lt;p&gt;区间编码可以看为算术编码的一个变种，比算术编码压缩效率略小，但运算速度近乎是算术编码的两倍。&lt;/p&gt;
&lt;p&gt;区间编码在整数（任意底）空间中进行进行计算，而算术编码的区间总是以小数的形式进行迭代。其他部分都几乎一样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727131132728.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;端到端熵模型&#34;&gt;端到端熵模型&lt;/h1&gt;
&lt;p&gt;Todo&amp;hellip;&lt;/p&gt;
&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/23834589&#34;&gt;算术编码（转载加笔记）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000011561822&#34;&gt;算数编码原理解析&lt;/a&gt;&lt;/p&gt;
- https://imfaye.me/post/entropy-coding/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>图像质量评价指标(MSE, PSNR, MS-SSIM)</title>
        <link>https://imfaye.me/post/image-quality-evaluation-metrics/</link>
        <pubDate>Thu, 02 Dec 2021 23:35:47 +0000</pubDate>
        
        <guid>https://imfaye.me/post/image-quality-evaluation-metrics/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/image-quality-evaluation-metrics/ -&lt;p&gt;如何评价重建图像的质量：比较重建图像与原始图像的可视误差。&lt;/p&gt;
&lt;h2 id=&#34;mse&#34;&gt;MSE&lt;/h2&gt;
&lt;p&gt;Mean Squared Error, 均方误差&lt;/p&gt;
&lt;p&gt;$MSE = \frac{1}{N}\sum\limits_{i=1}^{N}(x_i-y_i)^2$&lt;/p&gt;
&lt;p&gt;两者越接近，MSE 越小。MSE 损失的范围为 0 到 ∞。&lt;/p&gt;
&lt;h2 id=&#34;psnr&#34;&gt;PSNR&lt;/h2&gt;
&lt;p&gt;Peak Signal to Noise Ratio，峰值信噪比，即峰值信号的能量与噪声的平均能量之比，通常取 log 单位为分贝。&lt;/p&gt;
&lt;p&gt;$PSNR = 10 log_{10}\frac{MaxValue^2}{MSE}$&lt;/p&gt;
&lt;p&gt;从式子可以看出 PSNR 可以理解为 MSE 的另一种表达形式。与 MSE 相反的是，重建图像质量越好，PSNR 数值越大。&lt;/p&gt;
&lt;p&gt;对于图像来说，像素点数值以量化方式保存，八比特位深的情况，取值范围为 [0, 255]，$MaxValue$ 就是 255。&lt;/p&gt;
&lt;h2 id=&#34;ssim&#34;&gt;SSIM&lt;/h2&gt;
&lt;p&gt;MSE 与 PSNR 的问题是，在计算每个位置上的像素差异时，其结果仅与当前位置的两个像素值有关，与其它任何位置上的像素无关。这种计算差异的方式仅仅将图像看成了一个个孤立的像素点，而忽略了图像内容所包含的一些视觉特征，特别是图像的局部结构信息。而图像质量的好坏极大程度上是一个主观感受，其中结构信息对人主观感受的影响非常之大。&lt;/p&gt;
&lt;p&gt;而 SSIM (Structural Similarity，结构相似性) 就试图解决这个问题&lt;/p&gt;
&lt;p&gt;SSIM 由三部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;亮度对比 平均灰度作为亮度测量： $\mu_x = \frac{1}{N}\sum\limits_{i=1}^{N}x_i$ 亮度对比函数： $l(x,y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2+\mu_y^2+C_1}$&lt;/li&gt;
&lt;li&gt;对比度对比 灰度标准差作为对比度测量： $\sigma_x={(\frac{1}{N-1}\sum\limits_{i=1}^N{(x_i-\mu_x)}^2)}^{\frac{1}{2}}$ 亮度对比函数： $c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}$&lt;/li&gt;
&lt;li&gt;结构对比 结构测量： $\frac{x-\mu_x}{\sigma_x}$ 结构对比函数： $s(x,y) = \frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y + C_3}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SSIM 函数：&lt;/p&gt;
&lt;p&gt;$SSIM(x,y)={[l(x,y)]}^\alpha \cdot {[c(x,y)]}^\beta \cdot {[s(x,y)]}^\gamma$&lt;/p&gt;
&lt;p&gt;$一般取 \alpha = \beta =\gamma=1$&lt;/p&gt;
&lt;p&gt;$SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_x\sigma_y+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2\sigma_y^2+C_2)}$&lt;/p&gt;
&lt;p&gt;下图是同样 MSE 的图片，仅仅做对比拉伸（灰度拉伸，增大图像灰度级的动态范围）、均值偏移，其实不怎么影响人眼对图像的理解，而模糊和压缩痕迹则影响较大，这些情况下 SSIM 就能更好地做出判断。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220723233900866.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ms-ssim&#34;&gt;MS-SSIM&lt;/h2&gt;
&lt;p&gt;SSIM 算法基于 HVS 擅长从图像中提取结构信息，并利用结构相似度计算图像的感知质量。但 SSIM 是一种单尺度算法，实际上正确的图像尺度取决于用户的观看条件，如显示设备分辨率、用户的观看距离等。&lt;/p&gt;
&lt;p&gt;单尺度的 SSIM 算法可能仅适用于某个特定的配置，为了解决该问题，MS-SSIM (Multi-scale structural similarity) 在 SSIM 算法的基础上提出了多尺度的结构相似性评估算法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220723233921947.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MS-SSIM 算法，L 表示低通滤波器，2↓ 表示采样间隔为 2 的下采样&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;原始图像的尺度为 1，最大尺度为 M，对 $scale=j$ 的尺度而言，其亮度、对比度、结构的相似性分布表示为 $l_j(x,y), c_j(x,y), s_j(x,y)$，MS-SSIM 的计算公式为：&lt;/p&gt;
&lt;p&gt;$MS-SSIM(x,y) = {[l_M(x,y)]}^{\alpha M} \cdot \prod\limits_{j=1}^M{[c_j(x,y)]}^{\beta j}{[s_j(x,y)]}^{\gamma j}$&lt;/p&gt;
&lt;p&gt;一般，令 $\alpha_j = \beta_j = \gamma_j$，$j \in [1, M]$，我们得到：&lt;/p&gt;
&lt;p&gt;$MS-SSIM(x,y) = {[l_M(x,y)]}^{\alpha M} \cdot \prod\limits_{j=1}^M{[c_j(x,y) \cdot s_j(x,y)]}^{\alpha j}$&lt;/p&gt;
- https://imfaye.me/post/image-quality-evaluation-metrics/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>AVS3 编码位流</title>
        <link>https://imfaye.me/post/avs3-bitstream/</link>
        <pubDate>Thu, 01 Apr 2021 17:28:29 +0000</pubDate>
        
        <guid>https://imfaye.me/post/avs3-bitstream/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/avs3-bitstream/ -&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;视频序列&lt;/strong&gt;是位流的最高层语法结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;帧&lt;/strong&gt;由一个亮度样本矩阵和两个色度样本矩阵构成。&lt;strong&gt;场&lt;/strong&gt;由构成帧的三个样本矩阵中相间的行构成。奇数行构成顶场，偶数行构成底场。&lt;/p&gt;
&lt;p&gt;视频序列头由视频序列起码码开始，后面跟着一串编码图像数据。序列头可在位流中重复出现，称为重复序列头。使用重复序列头的主要目的是支持对视频序列的随机访问。&lt;/p&gt;
&lt;p&gt;一副图像可以是一帧或一场，其编码数据由图像起始码开始，到序列起始码、序列结束码或下一个图像起始码结束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;片&lt;/strong&gt;是图像中的矩形区域，包含若干最大编码单元在图像内的部分，片之间不应重叠。&lt;/p&gt;
&lt;p&gt;图像划分为&lt;strong&gt;最大编码单元&lt;/strong&gt;，最大编码单元之间不应重叠，最大编码单元左上角的样本不应超出图像边界，最大编码单元右下角的样本可超出图像边界。&lt;/p&gt;
&lt;p&gt;最大编码单元划分为一个或多个&lt;strong&gt;编码单元&lt;/strong&gt;，由编码树决定。编码单元划分为一个或多个&lt;strong&gt;变换块&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;码流结构及语义描述&#34;&gt;码流结构及语义描述&lt;/h2&gt;
&lt;h3 id=&#34;视频序列&#34;&gt;视频序列&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210403170108.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;序列头 sequence_header&lt;/p&gt;
&lt;p&gt;视频序列起始码、档次标号、级别标号、知识位流标志、知识图像允许标志、知识位流重复序列头标志、逐行序列标志、场图像序列标志、水平尺寸、垂直尺寸、色度格式、样本精度、编码样本精度、宽高比、帧率代码、比特率低位、比特率高位、低延迟、时间层标识允许标志、位流缓冲区尺寸、最大解码图像缓冲区大小、参考图像队列 1 索引存在标志、参考图像队列相同标志、参考图像队列配置集数、默认活跃参考图像数、最大编码单元尺寸、最小编码单元尺寸、划分单元最大比例、编码树最大划分次数、最小四叉树尺寸、最大二叉树尺寸、最大扩展四叉树尺寸、加权量化允许标志、加权量化矩阵加载标志、二次变换允许标志、样值偏倚补偿允许标志、自适应修正滤波允许标志、仿射运动补偿允许标志、对称运动矢量差模式允许标志、脉冲编码调制模式允许标志、自适应运动矢量精度允许标志、候选历史运动信息数、帧内预测滤波允许标志、高级运动矢量表达模式允许标志、运动矢量精度扩展模式允许标志、色度两步预测模式允许标志、帧内衍生模式允许标志、衍生模式划分边长最大尺寸、基于位置的变换允许标志、图像重排序延迟、跨片环路滤波允许标志、片划分一致性标志、参考同位置片标志、统一片大小标志、片宽度、片高度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;视频编辑码和视频序列结束码&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;视频编辑码 video_edit_code&lt;/p&gt;
&lt;p&gt;紧跟其后的第一幅 I 图像后续的 B 图像可能因缺少参考图像而不能正确解码&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;视频序列结束码 video_sequence_end_code&lt;/p&gt;
&lt;p&gt;标识视频序列的结束。如果 POI（显示顺序索引），如果 POI 的值大于 $(2^{32}-1)$，位流中应插入一个视频序列结束码。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考图像队列配置集&lt;/p&gt;
&lt;p&gt;参考知识图像标志、知识图像索引标志、被参考的知识图像索引、参考图像数、参考图像 DOI 差值绝对值、参考图像 DOI 差值符号&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自定义加权量化矩阵&lt;/p&gt;
&lt;p&gt;加权量化矩阵系数&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;图像&#34;&gt;图像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;帧内预测图像头&lt;/p&gt;
&lt;p&gt;帧内预测图像起始码、BBV 延时、时间编码标志、时间编码、解码顺序索引、知识图像索引、时间层标识、图像输出延迟、引用参考图像队列配置集标志、引用参考图像队列配置集索引、BBV 检测次数、逐行帧标志、图像编码结构标志、顶场在先、重复首场、顶场场图像标志、固定图像量化因子、去块滤波禁用标志、去块滤波参数标志、$\alpha$ 和 C 索引的偏移、$\beta$ 索引的偏移、色度量化参数禁用标志、色度量化参数增量 Cb、色度量化参数增量 Cr、图像加权量化允许标志、图像加权量化数据加载索引、加权量化参数索引、加权量化矩阵模型、加权量化参数增量 1、加权量化参数增量 2、图像自适应修正滤波允许标志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;帧间预测图像头&lt;/p&gt;
&lt;p&gt;帧间预测图像起始码、随机访问正确解码标志、图像编码方式、活跃参考图像数重载标志、活跃参考图像数、仿射预测子块尺寸标志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;片&lt;/p&gt;
&lt;p&gt;片起始码、固定片量化银子标志、片量化因子、片样值偏移补偿允许标志、高级熵编码字节对齐填充位、最大编码单元量化参数增量、样值偏移补偿合并方式索引、样值偏移补偿模式、样值偏移补偿区间模式偏移绝对值、样值偏移补偿区间模式偏移值符号值、样值偏移补偿区间模式起始偏移子区间位置、样值偏移补偿区间模式起始偏移子区间位置差、样值偏移补偿模式偏移值、样值偏移补偿边缘模式类型、最大编码单元自适应修正滤波允许标志、熵编码最大编码单元填充位、片结束码&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;编码树&#34;&gt;编码树&lt;/h3&gt;
&lt;p&gt;四叉树划分标志、编码单元预测模式、二叉树扩展四叉树划分标志、二叉树扩展四叉树划分类型标志、二叉树扩展四叉树划分方向标志&lt;/p&gt;
&lt;h3 id=&#34;编码单元&#34;&gt;编码单元&lt;/h3&gt;
&lt;p&gt;跳过模式标志、高级运动矢量表达模式标志、仿射模式标志、直接模式标志、帧内编码单元标志、基础运动矢量索引、运动矢量偏移量索引、运动矢量方向索引、仿射运动矢量索引、衍生模式划分标志、衍生模式划分方向、水平四叉衍生模式划分标志、垂直四叉衍生模式划分标志、水平非对称衍生模式标志、仿射自适应运动矢量精度索引、自适应运动矢量精度索引、编码单元子类型索引、预测参考模式、对称运动矢量差标志、运动矢量精度扩展模式标识、帧内亮度预测模式、帧内色度预测模式、帧内预测滤波标志、L0 预测单元参考索引、L0 运动矢量水平分量差绝对值、L0 运动矢量垂直分量差绝对值、L0 运动矢量水平分量差符号值、L0 运动矢量垂直分量差符号值、仿射帧间模式L0 运动矢量水平分量差绝对值、仿射帧间模式 L0 运动矢量垂直分量差绝对值、仿射帧间模式 L0 运动矢量水平分量差符号值、仿射帧间模式 L0 运动矢量垂直分量差符号值、L1&amp;hellip;、变换块系数标志、基于位置的变换块标志、Cb 变换块编码模板、Cr 变换块编码模板、亮度变换块编码模板&lt;/p&gt;
- https://imfaye.me/post/avs3-bitstream/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>H.265/HEVC 预测编码 笔记</title>
        <link>https://imfaye.me/post/prediction-coding/</link>
        <pubDate>Wed, 31 Mar 2021 17:04:49 +0000</pubDate>
        
        <guid>https://imfaye.me/post/prediction-coding/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/prediction-coding/ -&lt;h2 id=&#34;视频预测编码技术&#34;&gt;视频预测编码技术&lt;/h2&gt;
&lt;p&gt;预测编码是指利用已编码的一个或几个样本值，根据某种模型或方法，对当前的样本值进行预测，并对样本真实值和预测值之间的差值进行编码。&lt;/p&gt;
&lt;h3 id=&#34;帧内预测编码&#34;&gt;帧内预测编码&lt;/h3&gt;
&lt;p&gt;随着离散余弦变换 (DCT) 在图像、视频编码中的广泛应用，帧内预测转为在频域进行，如相邻块 DC 系数的差分编码等。由 DCT 的性质可知，DC 系数仅能反映当前块像素值的平均大小，因此上述频域中基于 DC 系数的帧内预测无法反映出视频的纹理信息，这限制了频域帧内预测的发展。&lt;/p&gt;
&lt;p&gt;H.264/AVC 标准中使用基于块的空域帧内预测方法，规定了若干种预测模式，每种模式都对应一种纹理方向（DC 模式除外），当前块预测像素由其预测方向上相邻块的边界重建像素生成。该方法使得编码器能根据视频内容特征自适应地选择预测模式。&lt;/p&gt;
&lt;p&gt;H.264/AVC 使用拉格朗日率失真优化 (RDO) 进行模式选择。它为每一种模式计算其拉格朗日代价：
$$
J = D + \lambda \cdot R
$$
其中，$D$ 表示当前预测模式下地失真，$R$ 表示编码当前预测模式下所有信息（如变换系数、模式细腻些、宏块划分方式等）所需的比特数。&lt;strong&gt;最优的预测模式不一定满足残差最小，而应指残差信号经过其他编码模块后最终的编码性能最优。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;H.264/AVC 标准及后来的 FRExt 扩展层一共规定了 3 种大小的亮度帧内预测块：4 × 4、8 × 8 以及 16 × 16。其中 4 × 4 和 8 × 8 块包含 9 种预测模式，16 × 16 块包含 4 种预测模式。色度分量的帧内预测都是基于 8 × 8 大小的块进行的，也有 4 种预测模式。&lt;/p&gt;
&lt;h3 id=&#34;帧间预测编码&#34;&gt;帧间预测编码&lt;/h3&gt;
&lt;h4 id=&#34;帧间预测编码原理&#34;&gt;帧间预测编码原理&lt;/h4&gt;
&lt;p&gt;目前主要的视频编码标准帧间预测部分都采样了基于块的运动补偿技术。其主要原理是为当前图像的每个像素块在之前已编码图像中寻找一个最佳匹配块，该过程被称为&lt;strong&gt;运动估计 (Motion Estimation, ME)&lt;/strong&gt;。其中被参考的图像称为&lt;strong&gt;参考图像 (Reference Frame)&lt;/strong&gt;，参考块到当前像素块的位移称为&lt;strong&gt;运动向量 (Motion Vector, MV)&lt;/strong&gt;，当前像素块与参考块的差值称为&lt;strong&gt;预测残差 (Prediction Residual)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 H.261 标准中，P 图像的预测方式只有前向预测。但实际场景中往往会产生不可预测的运动和遮挡，因此当前图像可能在之后的图像中更容易找到匹配块。为此，MPEG-1 标准定义了第三类图像，B 图像。为了提高运动估计精度，MPEG-1 首次采用了半像素精度的运动估计，半像素位置的参考像素值可由双线性差值方法产生。&lt;/p&gt;
&lt;p&gt;面向数字广播电视的标准 MPEG-2 首次支持了隔行扫描视频。一帧图像包含两个场，顶场和底场，每个帧图像的宏块需要被拆分成两个 16 × 8 的块分别进行预测。H.263 标准沿用了 MPEG-1 的双向预测和半像素精度运动估计，并进一步发展了 MPEG-2 中将一个宏块分成更小的块进行预测的思想。&lt;/p&gt;
&lt;p&gt;H.264/AVC 标准规定了 7 种大小的运动补偿块，一个宏块内部允许存在不同大小块的组合。此外 H.264/AVC 还使用了 1/4 精度像素运动估计、多参考图像预测、加权预测以及空域/时域 MV 预测等。&lt;/p&gt;
&lt;h4 id=&#34;帧间预测编码关键技术&#34;&gt;帧间预测编码关键技术&lt;/h4&gt;
&lt;h5 id=&#34;1-运动估计&#34;&gt;1. 运动估计&lt;/h5&gt;
&lt;p&gt;在大多数视频序列中，相邻图像内容非常相似，不需要对每幅图像的全部信息编码，只需要将当前图像中运动物体的运动信息传给解码器。运动估计就是提取当前图像运动信息的过程。&lt;/p&gt;
&lt;p&gt;将图像分为不同大小的像素块，只要块大小选择合适，则各个块的运动形式可以看成是统一的，每个块的运动参数可以独立地估计，这就是常用地基于块地运动表示法。&lt;/p&gt;
&lt;p&gt;有几个核心问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;运动估计准则&lt;/p&gt;
&lt;p&gt;常用地匹配准则主要有最小均方误差 (MSE)、最小平均绝对误差 (MAD) 和最大匹配像素数 (MPC) 等。为了简化计算，一般使用绝对误差和 (SAD) 来代替 MAD。此外，最小变换域绝对误差和 (SATD) 也是一种性能优异的匹配准则。&lt;/p&gt;
&lt;p&gt;最小 SAD 准则不含乘除法，且便于硬件实现，因而使用最广泛。SAD 准则仅考虑了残差的大小，而未考虑编码运动信息所需的比特数。因此，H.264/AVC 编码器在运动估计过程中使用 RDO 来选择 MV。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;搜索算法&lt;/p&gt;
&lt;p&gt;常用的搜索算法有全搜索算法、二维对数搜索算法、三步搜索算法等。除全搜索算法，其余算法统称为快速算法。快速算法容易陷入局部最优点，为避免这一点，在搜索算法的每一步中尽量搜索更多的点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;亚像素精度运动估计&lt;/p&gt;
&lt;p&gt;亚像素精度运动估计意味着需要对参考图像进行插值，好的插值方法能大幅改善运动补偿的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;2-mv-预测&#34;&gt;2. MV 预测&lt;/h5&gt;
&lt;p&gt;在大多数图像和视频中，一个运动物体可能会覆盖多个运动补偿块，因此空间域相邻块的运动向量具有较强的相关性。若使用相邻已编码块对当前块 MV 预测，将二者差值进行编码，则会大幅减少编码 MV 所需的比特数。同时，由于物体运动具有连续性，因此相邻图像同一位置像素块的 MV 也具有一定相关性。H.264/AVC 使用了空域和时域两种 MV 的预测方式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;MV 空域预测&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/NMJ$SDFJ_%60NZL~05%7DP~67IA.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331212942.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MV 时域预测&lt;/p&gt;
&lt;p&gt;在 H.264/AVC 中，MV 时域预测主要针对 B Slice。主要有以下两种形式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当 B 图像的两个 MV 都来自同一个方向时（都来自当前图像之前的参考图像或之后的），其中一个 MV 可用另一个 MV 来预测&lt;/p&gt;
&lt;p&gt;设两参考图像 $ref_0$ 和 $ref_1$ 与当前图像的距离分别为 $l_0$ 和 $l_1$，二者 MV 分别为 $MV_0$ 和 $MV_1$，则 $MV_1$ 可由下式预测：
$$
MVP_1 = \frac{l_1}{l_0} MV_0
$$&lt;/p&gt;
&lt;p&gt;$$
MVD_1 = MV_1 - MVP_1
$$&lt;/p&gt;
&lt;p&gt;编码器只需要传输 $MVD_1$，解码器可按相同规则产生 $MV_1$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;直接模式 MV 预测&lt;/p&gt;
&lt;p&gt;H.264/AVC 为 B Slice 提供一种 Direct Mode。在该模式下，MV 可直接预测的出，无需传送 MV 差值。预测方式有时域空域两种。时域预测介绍如下。&lt;/p&gt;
&lt;p&gt;设两参考图像 $ref_0$ 和 $ref_1$ 分别位于当前图像的前方和后方，与当前图像的距离分别为 $l_0$ 和 $l_1$，且 $ref_1$ 中与当前块对应位置块有一个指向 $ref_0$ 的 MV，则当前图像的两个 MV 可计算如下：
$$
MV_0 = \frac{l_0}{l_0 + l_1}MV
$$
$$
MV_1 = MV_0 - MV
$$&lt;/p&gt;
&lt;p&gt;MV 时域预测主要运用了自然界物体匀速运动的思想。&lt;/p&gt;
&lt;p&gt;与 H.264 标准相比，H.265 剔除里 Merge 和 AMVP 两种先进的 MV 预测技术。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多参考图像及加权预测&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;帧内预测&#34;&gt;帧内预测&lt;/h2&gt;
&lt;h3 id=&#34;帧内预测模式&#34;&gt;帧内预测模式&lt;/h3&gt;
&lt;h4 id=&#34;亮度帧内预测模式&#34;&gt;亮度帧内预测模式&lt;/h4&gt;
&lt;p&gt;H.265/HEVC 亮度分量帧内预测支持 5 种大小的 PU，每一种大小的 PU 都对应 35 种预测模式，包括 Planar 模式、DC 模式以及 33 种角度模式。所有预测模式都使用相同的模板。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/%5D5%25KJ%7BTWLY0XF107LI%7DGWLD.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Planar 模式&lt;/p&gt;
&lt;p&gt;由 H.264/AVC 中的 Plane 模式发展而来，适用于像素值缓慢变化的区域。使用水平和垂直方向两个线性滤波器，并将二者的平均值作为当前块像素的预测值。这一做法能使预测像素平缓变化，与其他模式相比能提升视频的主观质量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DC 模式&lt;/p&gt;
&lt;p&gt;适用于大面积平坦区域。当前块预测值可由其左侧和上方（不包含左上角、左下方和右上方）参考像素的平均值得到。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;角度模式&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;亮度模式的编码&#34;&gt;亮度模式的编码&lt;/h4&gt;
&lt;p&gt;H.265/HEVC 标准建立了一个帧内预测模式候选列表 candModeList，表中有 3 个候选预测模式，用于存储相邻 PU 的预测模式。&lt;/p&gt;
&lt;h4 id=&#34;色度模式的编码&#34;&gt;色度模式的编码&lt;/h4&gt;
&lt;p&gt;共有 5 种模式：Planar 模式、垂直模式、水平模式、DC 模式以及对应亮度分量的预测模式。若对应亮度预测模式为前四种之一，则替换为角度预测中的模式 34。&lt;/p&gt;
&lt;h3 id=&#34;帧内预测过程&#34;&gt;帧内预测过程&lt;/h3&gt;
&lt;p&gt;在 H.265/HEVC 中，35 种预测模式是在 PU 的基础上定义的，而具体帧内预测过程的实现则是以 TU 为单位的。标准规定 PU 可以以四叉树的形式划分 TU，且一个 PU 内的所有 TU 共享一种预测模式。&lt;/p&gt;
&lt;p&gt;帧内预测可分为以下 3 个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断当前 TU 相邻参考像素是否可用（边界或未编码的就不可用）并作相应处理&lt;/li&gt;
&lt;li&gt;对参考像素进行滤波&lt;/li&gt;
&lt;li&gt;根据滤波后的参考像素计算当前 TU 的预测像素值&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;帧间预测&#34;&gt;帧间预测&lt;/h2&gt;
&lt;h3 id=&#34;运动估计&#34;&gt;运动估计&lt;/h3&gt;
&lt;h4 id=&#34;搜索算法&#34;&gt;搜索算法&lt;/h4&gt;
&lt;p&gt;在基于块运动补偿的视频编码框架中，运动搜索是最为重要的环节之一，也是编码器最耗时的模块。H.265/HEVC 官方测试编码器 HM10.0 给出了两种搜索算法：全搜索算法和 TZSearch 算法。&lt;/p&gt;
&lt;h4 id=&#34;亚像素精度运动估计&#34;&gt;亚像素精度运动估计&lt;/h4&gt;
&lt;h3 id=&#34;mv-预测技术&#34;&gt;MV 预测技术&lt;/h3&gt;
&lt;p&gt;H.265/HEVC 在 MV 预测方面提出了两种新技术，Merge 技术和 AMVP 技术。二者区别主要体现于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Merge 可以看成一种编码模式，在该模式下，当前 PU 的 MV 直接由空域或时域上邻近的 PU 预测得到，不存在 MVD (MV Difference)；而 AMVP 可以看成一种 MV 预测技术，编码器只需要对实际 MV 与预测 MV的差值进行编码，因此存在 MVD。&lt;/li&gt;
&lt;li&gt;二者 MV 候选列表长度不同，构建候选 MV 列表的方式也有所区别&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;merge-模式&#34;&gt;Merge 模式&lt;/h4&gt;
&lt;p&gt;为当前 PU 建立一个 MV 候选列表，列表中存在 5 个候选 MV（及其对应的参考图像），通过遍历这 5 个候选 MV，并进行率失真代价的计算，选取率失真代价最小的一个作为该 Merge 模式的最优 MV。若编/解码端按相同的方式键立该候选列表，则编码器只需要传输最优 MV 在候选列表中的索引即可。&lt;/p&gt;
&lt;p&gt;Merge 模式建立的 MV 候选列表包含时域和空域两种情形，对于 B Slice，还包含组合列表的方式。&lt;/p&gt;
&lt;h5 id=&#34;空域候选列表的建立&#34;&gt;空域候选列表的建立&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/NMJ$SDFJ_%60NZL~05%7DP~67IA.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;时域候选列表的建立&#34;&gt;时域候选列表的建立&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331212942.pn&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;组合列表的建立&#34;&gt;组合列表的建立&lt;/h5&gt;
&lt;p&gt;对于 B Slice 中的 PU 而言，由于存在两个 MV，因此其 MV 候选列表也需要提供两个预测 MV。H.265/HEVC 将 MV 候选列表中的前 4 个 MV 进行两两组合，产生了用于 B Slice 的组合列表。&lt;/p&gt;
&lt;h4 id=&#34;amvp-技术&#34;&gt;AMVP 技术&lt;/h4&gt;
&lt;p&gt;高级运动向量预测 (Advanced Motion Vector Prediction, AMVP) 为当前 PU 建立候选 MV 列表，编码器从中最优的预测 MV，并对 MV 进行差分编码；解码端通过建立相同的列表，仅需要将 MVD 与预测 MV 在该列表中的序号即可计算出当前 PU 的 MV。&lt;/p&gt;
&lt;p&gt;类似于 Merge 模式，AMVP 候选 MV 列表也包含空域和时域两种情形，不同的是 AMVP 列表长度仅为 2。&lt;/p&gt;
&lt;h3 id=&#34;加权预测&#34;&gt;加权预测&lt;/h3&gt;
&lt;p&gt;加权预测可用于修正 P Slice 或 B Slice 中的运动补偿预测像素。H.265/HEVC 中介绍了两种加权预测方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;默认加权预测&lt;/p&gt;
&lt;p&gt;未使用权值 $\omega$，根据参考图像队列的不同分 3 种情况计算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explicit 加权预测&lt;/p&gt;
&lt;p&gt;其权值 $\omega$ 由编码器决定，并需要传送至解码端。也分 3 种情况。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;pcm-模式&#34;&gt;PCM 模式&lt;/h2&gt;
&lt;p&gt;在 PCM 模式下，编码器直接传输一个 CU 的像素值，而不经过预测、变换等其他操作。&lt;/p&gt;
&lt;p&gt;对于一些特殊情况，例如当图像的内容极不规则或量化参数非常小时，该模式与传统的“帧内 - 变换 - 量化 - 熵编码”相比，效率会更高。此外，PCM 模式还适用于无损编码情形。&lt;/p&gt;
- https://imfaye.me/post/prediction-coding/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>H.265/HEVC 编码结构 笔记</title>
        <link>https://imfaye.me/post/video-coding-structure/</link>
        <pubDate>Wed, 31 Mar 2021 11:34:13 +0000</pubDate>
        
        <guid>https://imfaye.me/post/video-coding-structure/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/video-coding-structure/ -&lt;h2 id=&#34;名词一览&#34;&gt;名词一览&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;GOP (Group of Pictures) - 图像组&lt;/li&gt;
&lt;li&gt;IDR (Instantaneous Decoding Refresh) - 即时解码刷新&lt;/li&gt;
&lt;li&gt;Slice - 片&lt;/li&gt;
&lt;li&gt;SS (Slice Segment) - 片段&lt;/li&gt;
&lt;li&gt;CTU (Coding Tree Unit) - 树形结构单元&lt;/li&gt;
&lt;li&gt;CTB (Coding Tree Block) - 树形编码块&lt;/li&gt;
&lt;li&gt;CU (Coding Unit) - 编码单元&lt;/li&gt;
&lt;li&gt;SPS (Sequence Parameter Set) - 序列参数集&lt;/li&gt;
&lt;li&gt;PPS (Picture Parameter Set) - 图像参数集&lt;/li&gt;
&lt;li&gt;CVS (Coded Video Sequence) - 一个 GOP 编码后生成的压缩数据&lt;/li&gt;
&lt;li&gt;VPS (Video Parameter Set) - 视频参数集&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;编码结构概述&#34;&gt;编码结构概述&lt;/h2&gt;
&lt;h3 id=&#34;编码结构&#34;&gt;编码结构&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;视频序列&lt;/strong&gt;分隔为若干个图像组 (&lt;strong&gt;GOP&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;存在两种 GOP 类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;封闭式 GOP&lt;/p&gt;
&lt;p&gt;每一个 GOP 以 IDR 图像开始，各个 GOP 之间独立编解码。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;开放式 GOP&lt;/p&gt;
&lt;p&gt;第一个 GOP 中的第一个帧内编码图像为 IDR 图像，后续 GOP 中的第一个帧内编码图像为 non-IDR 图像。后面 GOP 中的帧间编码图像可以使用前一个 GOP 的已编码图像作为参考图像。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331120920.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;每个 GOP 又被分为多个片 (&lt;strong&gt;Slice&lt;/strong&gt;)，片与片之间独立编解码。主要目的之一是在数据丢失的情况下进行重新同步。&lt;/p&gt;
&lt;p&gt;每个片由一个或多个片段 (&lt;strong&gt;SS&lt;/strong&gt;, Slice Segment) 组成。&lt;/p&gt;
&lt;p&gt;树形结构单元 (CTU) 类似传统的宏块。每个 CTU 包括一个亮度 CTB 和两个色差 CTB。&lt;/p&gt;
&lt;p&gt;一个 SS 在编码时，先被分割为相同大小的 &lt;strong&gt;CTU&lt;/strong&gt;，每一个 CTU 按照四叉树分割方式被划分为不同类型的 &lt;strong&gt;CU&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331120838.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;以上即为编码时的分层处理架构。&lt;/p&gt;
&lt;h3 id=&#34;码流结构&#34;&gt;码流结构&lt;/h3&gt;
&lt;p&gt;码流结构上，H.265/HEVC 压缩数据采用了类似于 H.264/AVC 的分层结构。&lt;/p&gt;
&lt;p&gt;将属于 GOP 层、Slice 层中共用的大部分语法元素游离出来，组成 SPS 和 PPS。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SPS&lt;/strong&gt; 中包含了一个 CVS 中所有图像共用的信息。SPS 中大致包括解码相关信息，如档次级别、分辨率、某档次中编码工具开关标识和涉及的参数、时域可分级信息等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PPS&lt;/strong&gt; 中包含一副图像所用的公共参数，大致包括初始图像控制信息，如初始量化参数、分块信息等。一副图像中所有 SS 引用同一个 PPS。&lt;/p&gt;
&lt;p&gt;此外，为了兼容在其他应用上的扩展，H.265/HEVC 的语法架构中增加了 &lt;strong&gt;VPS&lt;/strong&gt;，其内容大致包括多个子层共享的语法元素，其他不属于 SPS 的特定信息等。&lt;/p&gt;
&lt;p&gt;对于一个 SS，通过引用它所使用的 PPS，该 PPS 又引用对应的 SPS，该 SPS 又引用对应的 VPS，最终得到 SS 的公用信息。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331120942.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;片段层&#34;&gt;片段层&lt;/h2&gt;
&lt;p&gt;一副图像可以被分割为一个或多个 Slice，每个 Slice 的压缩数据都是独立的，Slice 头信息无法通过前一个 Slice 的头信息推断得到。这就要求 Slice 不能跨过它的边界来进行帧内或帧间预测。&lt;/p&gt;
&lt;p&gt;根据编码类型不同，Slice 可分为以下几部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I Slice&lt;/p&gt;
&lt;p&gt;该 Slice 中所有 CU 的编码过程都使用帧内预测&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P Slice&lt;/p&gt;
&lt;p&gt;在 I Slice 的基础上，该 Slice 中的 CU 还可以使用帧间预测，每个 PB（预测块）使用至多一个运动补偿预测信息。P Slice 只使用图像参考列表 list 0。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;B Slice&lt;/p&gt;
&lt;p&gt;在 P Slice 的基础上，该 Slice 中的 CU也可以使用帧间预测，每个 PB（预测块）可以使用至多两个运动补偿预测信息。B Slice 可以使用图像参考列表 list 0 和 list 1。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个独立的 Slice 可以被进一步划分为若干个 SS，包括一个独立 SS 和若干个依赖 SS，并且以独立 SS 作为该 Slice 的开始。&lt;/p&gt;
&lt;p&gt;一个 SS 包含整数个 CTU（至少一个），并且这些 CTU 分布在同一个 NAL 单元中。SS 可以作为一个分组来传送视频编码数据。&lt;/p&gt;
&lt;h2 id=&#34;tile-单元&#34;&gt;Tile 单元&lt;/h2&gt;
&lt;h3 id=&#34;tile-单元描述&#34;&gt;Tile 单元描述&lt;/h3&gt;
&lt;p&gt;一副图像不仅可以划分为若干个 Slice，也可以划分为若干个 Tile。即从水平和垂直方向将一个图像分割为若干个矩形区域，一个矩形区域就是一个 Tile。每个 Tile 包含整数个 CTU。&lt;/p&gt;
&lt;p&gt;Tile 提供比 CTB 更大程度的并行，在使用时无须进行复杂的线程同步。&lt;/p&gt;
&lt;p&gt;在同一幅图像中，可以存在某些 Slice 中包含多个 Tile 和某些 Tile 包含多个 Slice 的情况。&lt;/p&gt;
&lt;h3 id=&#34;slice-和-tile&#34;&gt;Slice 和 Tile&lt;/h3&gt;
&lt;p&gt;Tile 形装基本上为矩形，Slice 为条带状。&lt;/p&gt;
&lt;p&gt;Slice 由一系列 SS 组成，一个 SS 由一系列 CTU 组成。Tile 则直接由一系列 CTU 组成。&lt;/p&gt;
&lt;p&gt;每个 Slice/SS 和 Tile 至少要满足以下两个条件之一：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;一个 Slice/SS 中的所有 CTU 属于同一个 Tile&lt;/li&gt;
&lt;li&gt;一个 Tile 中的所有 CTU 属于同一个 Slice/SS&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;树形编码块&#34;&gt;树形编码块&lt;/h2&gt;
&lt;p&gt;传统的视频编码基于宏块实现。考虑到高清视频 / 超高清视频的自身特性，H.265/HEVC 标准中引入了树形编码单元 CTU，其尺寸由编码器指定，且可大于宏块尺寸。&lt;/p&gt;
&lt;p&gt;同一位置处的一个亮度 CTB 和两个色度 CTB，再加上相应的语法元素形成一个 CTU。在高分辨率视频的编码中，使用较大的 CTB 可以获得更好的压缩性能。&lt;/p&gt;
&lt;p&gt;H.265/HEVC 为图像划分定义了一套全新的语法单元，包括编码单元 (CU)、预测单元 (PU)、变换单元  (TU)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CU 是进行预测、变换、量化和熵处理等处理的基本单元&lt;/li&gt;
&lt;li&gt;PU 是进行帧内/帧间预测的基本单元&lt;/li&gt;
&lt;li&gt;TU 是进行变换和量化的基本单元&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;编码单元&#34;&gt;编码单元&lt;/h3&gt;
&lt;p&gt;大尺寸图像的一个特点是平缓区域的面积更大，用较大的块编码能极大提升编码效率。在 H.264/AVC 中，编码块的大小是固定的。而在 H.265/HEVC 中，一个 CTB 可以直接作为一个 CB，也可以进一步以四叉树的形式划分为多个小的 CB。大的 CB 可以使得平缓区域的编码效率提高，小 CB 能很好地处理图像局部的细节。&lt;/p&gt;
&lt;p&gt;编码单元是否继续划分取决于分割标志位 Split Flag。&lt;/p&gt;
&lt;h3 id=&#34;预测单元&#34;&gt;预测单元&lt;/h3&gt;
&lt;p&gt;预测单元规定了编码单元的所有预测模式。帧内预测的方向、帧间预测的分割方式、运动矢量预测、帧间预测参考图像索引号都属于预测单元的范畴。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331153821.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;变换单元&#34;&gt;变换单元&lt;/h3&gt;
&lt;p&gt;TU 的大小依赖于 CU 模式，在一个 CU 内，允许 TU 跨越多个 PU，以四叉树的形式递归划分。对于一个 2N × 2N 的 CU，有一个标志位决定其是否划分为 4 个 N × N 的 TU，是否可以进一步划分由 SPS 中的 TU 最大划分深度决定。&lt;/p&gt;
&lt;h2 id=&#34;档次层和级别&#34;&gt;档次、层和级别&lt;/h2&gt;
&lt;p&gt;在 H.264 中就有对档次 (Profile) 和级别 (Level) 的划分，它们规定了比特流必须遵守的一些限制要求。而 H.265/HEVC 中在此基础上又新定义了一个概念：层 (Tile)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Profile 主要规定编码器可采用哪些编码工具或算法&lt;/li&gt;
&lt;li&gt;Level 是指根据解码端的负载和存储空间情况对关键参数加以限制&lt;/li&gt;
&lt;li&gt;有些 Level 定义了两个 Tile: 主层 (Main Tile) 和高层 (High Tile)，主层用于大多数应用，高层用于那些最苛刻的应用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;满足某一 Level 和 Tile 的解码器应当可以解码当前以及比当前更低的 Level 和 Tile 的所有码流。&lt;/p&gt;
&lt;p&gt;满足某一 Profile 的解码器必须支持该 Profile 中的所有特性。编码器不必实现 Profile 中的所有特性，但生成的码流必须遵守标准规定。&lt;/p&gt;
- https://imfaye.me/post/video-coding-structure/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>多媒体基础知识</title>
        <link>https://imfaye.me/post/multimedia-basics/</link>
        <pubDate>Mon, 29 Mar 2021 15:10:25 +0000</pubDate>
        
        <guid>https://imfaye.me/post/multimedia-basics/</guid>
        <description>PIKA☆NCHI https://imfaye.me/post/multimedia-basics/ -&lt;h2 id=&#34;图像数值表示&#34;&gt;图像数值表示&lt;/h2&gt;
&lt;h3 id=&#34;分辨率&#34;&gt;分辨率&lt;/h3&gt;
&lt;p&gt;分辨率的基础单位是像素。1280 * 720 P 的分辨率代表共有 1280 * 720 个像素点。&lt;/p&gt;
&lt;p&gt;一台物理设备出厂时就已经定下了它所能拥有的最大像素点是多少。电脑显示屏调整分辨率是系统通过运算来给出模拟色彩块填充适配的。&lt;/p&gt;
&lt;p&gt;下述像素排列方式内容为 expansion pack，可略过。&lt;/p&gt;
&lt;h3 id=&#34;像素排列方式&#34;&gt;像素排列方式&lt;/h3&gt;
&lt;h4 id=&#34;标准-rgb-排列&#34;&gt;标准 RGB 排列&lt;/h4&gt;
&lt;p&gt;LCD 屏幕上常采用标准 RGB 排列，会将一个像素分为 3 个子像素并排排列，通过红、绿、蓝滤色片将 LCD 背光模组的白光过滤后形成相应的 RGB 子像素排列。当需要显示不同颜色的时候，3 个子像素以不同的亮度发光，在视觉上会混合成所需要的颜色。&lt;/p&gt;
&lt;h4 id=&#34;pentile-排列&#34;&gt;PenTile 排列&lt;/h4&gt;
&lt;p&gt;PenTile 排列多见于 OLED 屏幕上，因子像素呈现钻石排列而得名。PenTile 排列的每个像素由红、绿和蓝、绿子像素组合而成，绿色像素是完整的，而红蓝像素相比传统 RGB 排列各减少二分之一，子像素总数减少了约三分之一。&lt;/p&gt;
&lt;p&gt;不像标准 RGB 排列每个像素更加独立，PenTile 排列在显示许多内容时需要借用相邻像素，显示精细内容时同分辨率下相较标准 RGB 排列的屏幕细腻度不足。&lt;/p&gt;
&lt;p&gt;蓝色 OLED 的发光效率要比红色和绿色低，达到相同的发光强度必须使用更高的通过电流，因而蓝色像素衰减速度更快，也就会加速“烧屏”现象的产生。&lt;/p&gt;
&lt;h3 id=&#34;yuv-表示方式&#34;&gt;YUV 表示方式&lt;/h3&gt;
&lt;p&gt;对于视频裸数据而言，更多使用 YUV 数据格式显示。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Y 表示明亮度 (Luminance / Luma)，即灰阶值&lt;/li&gt;
&lt;li&gt;U、V 表示色度 (Chrominance / Chroma)，描述色调饱和度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;亮度通过 RGB 输入信号来建立，方法是将 RGB 信号的特定部分叠加到一起。色度定义了颜色的色彩和饱和度，分别用 Cr 和 Cb 表示。Cr 表示 RGB 输入信号红色部分与 RGB 亮度值之间的差异，Cb 表示 RGB 输入信号蓝色部分与 RGB 信号亮度值之间的差异。&lt;/p&gt;
&lt;h4 id=&#34;yuv-优点&#34;&gt;YUV 优点&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;与黑白电视机也能兼容&lt;/p&gt;
&lt;p&gt;Y 和 UV 分量分离，只有 Y 分量就是黑白图像。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;相较于 RGB，YUV 数据格式占用空间小&lt;/p&gt;
&lt;p&gt;人眼对色度的敏感程度低于对亮度的敏感程度（因为识别亮度的视网膜杆细胞比识别色度的视网膜锥细胞多）。将色的信息减少，人眼也无法察觉。且并不是每个像素点都需要包含 YUV 三个分量，根据不同的采用格式，每个 Y 分量可以对应自己的 UV 分量。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;yuv-采样格式&#34;&gt;YUV 采样格式&lt;/h4&gt;
&lt;h5 id=&#34;yuv-444-采样&#34;&gt;YUV 4:4:4 采样&lt;/h5&gt;
&lt;p&gt;每个像素三个分量信息完整。&lt;/p&gt;
&lt;p&gt;举个例子，一张 1 * 4 的图片：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]&lt;/p&gt;
&lt;p&gt;采样码流为：Y0 U0 V0 Y1 U1 V1 Y2 U2 V2 Y3 U3 V3&lt;/p&gt;
&lt;p&gt;最后映射出的像素点依旧为 [Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一张 1280 * 720P 的图片使用 YUV 4:4:4 采样，大小为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1280 * 720 * 3 / 1024 / 1024 = 2.636 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&#34;yuv-422-采样&#34;&gt;YUV 4:2:2 采样&lt;/h5&gt;
&lt;p&gt;Y 分量和 UV 分量按 2:1 的比例采样。&lt;/p&gt;
&lt;p&gt;举个例子，一张 1 * 4 的图片：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]&lt;/p&gt;
&lt;p&gt;采样码流为：Y0 U0 Y1 V1 Y2 U2 Y3 V3&lt;/p&gt;
&lt;p&gt;最后映射出的像素点依旧为 [Y0 U0 V1]、[Y1 U0 V1]、[Y2 U2 V3]、[Y3 U2 V3]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一张 1280 * 720 P 的图片使用 YUV 4:2:2 采样，大小为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(1280 * 720 + 1280 * 720 * 0.5 * 2) / 1024 / 1024 = 1.759 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h5 id=&#34;yuv-420-采样&#34;&gt;YUV 4:2:0 采样&lt;/h5&gt;
&lt;p&gt;不是指没有 Cb，而是意味着第一行 Y 分量和 U 分量按 2:1 的比例采样，第二行 Y 分量和 V 分量按 2:1 的比例采样。Y 分量和 UV 分量按 4:1 的比例采样。&lt;/p&gt;
&lt;p&gt;举个例子，一张 2* 4 的图片：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;图像像素为：[Y0 U0 V0]、[Y1 U1 V1]、[Y2 U2 V2]、[Y3 U3 V3]、[Y4 U4 V4]、[Y5 U5 V5]、[Y6 U6 V6]、[Y7 U7 V7]&lt;/p&gt;
&lt;p&gt;采样码流为：Y0 U0 Y1 Y2 U2 Y3 Y4 V4 Y5 Y6 V6 Y7&lt;/p&gt;
&lt;p&gt;最后映射出的像素点依旧为 [Y0 U0 V4]、[Y1 U0 V4]、[Y2 U2 V6]、[Y3 U2 V6]、[Y4 U0 V4]、[Y5 U0 V4]、[Y6 U2 V6]、[Y7 U2 V6]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一张 1280 * 720 P 的图片使用 YUV 4:2:0 采样，大小为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(1280 * 720 + 1280 * 720 * 0.25 * 2) / 1024 / 1024 = 1.318 MB
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;yuv-存储格式&#34;&gt;YUV 存储格式&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;planar 平面格式&lt;/p&gt;
&lt;p&gt;先连续存储所有像素点的 Y 分量，然后存储 UV 分量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;packed 打包模式&lt;/p&gt;
&lt;p&gt;每个像素点的 YUV 分量连续交替存储。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;rgb-和-yuv-转化&#34;&gt;RGB 和 YUV 转化&lt;/h4&gt;
&lt;p&gt;对于图像显示器来说，它是通过 RGB 模型显示图像的，而在传输图像数据时又是使用 YUV 模型的。因此两种模型需要互相转化。&lt;/p&gt;
&lt;p&gt;有如下公式：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Y = 0.299 * R + 0.587 * G + 0.114 * B
U = - 0.147 * R - 0.289 * G + 0.436 * B
V = 0.615 * R - 0.515 * G - 0.100 * B
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;R = Y + 1.14 * V
G = Y - 0.39 * U - 0.58 * V
B = Y + 2.03 * U
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ibp-帧&#34;&gt;IBP 帧&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I 帧 (intra picture)&lt;/p&gt;
&lt;p&gt;内部编码帧（也称为关键帧），通常是每个 GOP 片段的第一帧，经过适度压缩，作为随机访问的参考点，可以当作静态图像。I 帧压缩可去掉视频的空间冗余信息。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P 帧 (predictive-frame)&lt;/p&gt;
&lt;p&gt;前向预测编码帧（也称为预测帧），通过将图像序列中前面已编码帧的时间冗余信息去充分去除压缩传输数据量的编码图像，需要参考前面的一个 I 帧或者 P 帧才能解码成一张完整的图像。P 帧可以简单理解为当前帧画面与前一帧的差别。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;B 帧 (bi-directional interpolated prediction frame)&lt;/p&gt;
&lt;p&gt;双向预测内插编码帧（也称双向预测帧），需要参考前面的一个 I 帧或者 P 帧以及后面的一个 P 帧才能编码成一张完整的图像。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单来说，I 帧是一个完整的画面，而 P 帧和 B 帧记录的是相对于 I 帧的变化。如果没有 I 帧，P 帧和 B 帧就无法解码。压缩比 I 帧 &amp;lt; P 帧 &amp;lt; B 帧。&lt;/p&gt;
&lt;h2 id=&#34;gop&#34;&gt;GOP&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210329171222.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;编码器将多张图像进行编码后生产成一段一段的 GOP (Group of Pictures)，解码器在播放时则是读取一段一段的 GOP 进行编码后读取画面再渲染显示。GOP 是一组连续的画面，由一张 I 帧和数张 B / P 帧组成，是视频图像编码器和解码器存取的基本单位。gop_size 描述的是两个 I 帧之间的帧数目。&lt;/p&gt;
&lt;h2 id=&#34;码率和帧率&#34;&gt;码率和帧率&lt;/h2&gt;
&lt;h3 id=&#34;帧率-frame-rate&#34;&gt;帧率 (Frame Rate)&lt;/h3&gt;
&lt;p&gt;表示每秒实践显示的帧数（Frames per Second，简称 FPS）。&lt;/p&gt;
&lt;p&gt;对于人眼来说，如果所看画面的帧率高于 24，就会认为是连贯的，此现象称为视觉暂留。&lt;/p&gt;
&lt;h3 id=&#34;码率比特率&#34;&gt;码率（比特率）&lt;/h3&gt;
&lt;p&gt;码率指每秒传输的比特数，单位为 bps (Bits Per Second)，通俗一点讲就是取样率，单位时间内取样率越大，精度就越高，处理出的文件就越接近原始文件。&lt;/p&gt;
&lt;p&gt;文件体积与取样率成正比，所有的编码格式都很重视如何用最低的码率达到最少的失真。&lt;/p&gt;
&lt;p&gt;码率简单来说是指再压缩视频时给这个视频指定一个参数，用以告诉压缩软件&lt;strong&gt;期望的压缩后视频的大小&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;视频基础知识扫盲&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://glumes.com/post/ffmpeg/understand-yuv-format/&#34;&gt;一文读懂 YUV 的采样与格式&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.yuque.com/webmedia/handbook/ibp&#34;&gt;IBP帧&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
- https://imfaye.me/post/multimedia-basics/ - Faye</description>
        </item>
    
    
  </channel>
</rss> 