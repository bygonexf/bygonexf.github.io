<?xml-stylesheet href="/rss.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fxye.</title>
    <link>https://bygonexf.github.io/</link>
    <description>Recent content on Fxye.</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Faye</copyright>
    <lastBuildDate>Sat, 25 Feb 2023 23:25:36 +0800</lastBuildDate>
    
        <atom:link href="https://bygonexf.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>2018年9月，我想记下的旧家</title>
        <link>https://bygonexf.github.io/post/nolstalgia2018/</link>
        <pubDate>Sat, 25 Feb 2023 23:25:36 +0800</pubDate>
        
        <guid>https://bygonexf.github.io/post/nolstalgia2018/</guid>
        <description>Fxye. https://bygonexf.github.io/post/nolstalgia2018/ -&lt;p&gt;备忘录里2018年9月3日记下的段落。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;被告知寒假回来后可能要搬到新家、甚是惶恐、遂用相机和纸笔大刀阔斧地记下周遭全貌和牵扯进的重要回忆。&lt;/p&gt;
&lt;p&gt;以下陈列一些过程中还算特别的细枝末节。&lt;/p&gt;
&lt;p&gt;相较而言奇怪地多的物件，一是电脑，二是风铃。（大学以前，好像送人生日礼物，不是书和文具，就是风铃）。&lt;/p&gt;
&lt;p&gt;床头柜至今仍贴着“明石浦朝雾蒙蒙，岛隐思行船”的便签，貌似是小学在图书馆借了一本奇怪的书说睡前默念三遍第二天可以早起的咒语。&lt;/p&gt;
&lt;p&gt;从前在家的时候，每年圣诞节都会买装饰物挂在卧室的灯上，春节也会挂，然而涉及到生肖一年到头便会换，圣诞买的却一直留下来了，以至于看自己卧室的灯看出圣诞树的感觉（彩条雪球星星雪花风铃&amp;hellip;全有）。&lt;/p&gt;
&lt;p&gt;书房的印象是墙上的中国和世界地图（伦敦处画了个红圈），和我打印的贝克街照片（那时我的梦想还是当个侦探，现在&amp;hellip;其实想想本质对杂学家的兴趣也没变）。&lt;/p&gt;
&lt;p&gt;屯了一柜子的东西，除了正经书（这大概是好几柜子&amp;hellip;），都有被我妈无情丢弃的趋势，高中去南京读书回来发现自己一柜子精挑细选从小学留下来的少年文艺儿童文学全被当废纸卖了非常心痛，大学回来发现又一柜子的推理杂志和书也无踪无影了。多少有些可惜。&lt;/p&gt;
&lt;p&gt;很喜欢火车经过的声音，睡不着时那种宫泽贤治的银河铁道之夜似的美感非常治愈。&lt;/p&gt;
&lt;p&gt;周遭极适合探险，小学和邻居干过的熊孩子事包括但不仅限：烧草坪和莫名其妙从楼上砸下来的一袋豆腐、利用凸透镜聚光生火灼树叶做出艺术品、暴雪那年越过铁门跑到铁路上尽享无人踩踏的大片雪地、卧（废弃的）轨。&lt;/p&gt;
&lt;p&gt;高中时模考写过一次巷子，范文印出来被隔壁班镇江老乡看见说想去我家参观。&lt;/p&gt;
&lt;p&gt;什么扑簌扑簌的泡桐花、神秘的塔楼、满面高墙的爬山虎、一路毛茸茸的狗尾巴草和蒲公英、远山和青松、街巷文化里的烟火气、都是真的。&lt;/p&gt;
&lt;p&gt;现实是那篇文章描述之上的美好程度。&lt;/p&gt;
&lt;p&gt;谨以彼时作文、未公开的诗作、拍下并用力记住的照片，不成体统地感谢给予我完美童年的这个家。&lt;/p&gt;
- https://bygonexf.github.io/post/nolstalgia2018/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>我所喜欢的河濑直美</title>
        <link>https://bygonexf.github.io/post/kawase-naomi/</link>
        <pubDate>Sun, 15 Jan 2023 15:04:21 +0800</pubDate>
        
        <guid>https://bygonexf.github.io/post/kawase-naomi/</guid>
        <description>Fxye. https://bygonexf.github.io/post/kawase-naomi/ -&lt;h2 id=&#34;最近偏爱河濑直美&#34;&gt;最近偏爱河濑直美&lt;/h2&gt;
&lt;p&gt;很久之前就想看萌之朱雀，那段时间很喜欢超8mm摄像机的影片，目测剧情觉得相当工整并没第一时间看。前段时间偶然扫过沙罗双树的剧情简介，古都，双生子，神隐，祭典，这些词的意象以及剧照都相当吸引我。看完之后就开始沉迷河濑直美。&lt;/p&gt;
&lt;p&gt;河濑直美确实是“一生只拍一部电影”的典型，始终徘徊在身世问题左右。想起她在东大入学仪式的致辞，也说了一直一直只盯着一扇窗，这一扇窗和世界的联结又也能让她和世界上众多的人相遇。身世，原生家庭，童年，似乎很多人长久执着于这些母题，有的时候我在想，是不是就算长大以后，我们作为人类的理解力也就只能帮助我们消化自我到青春期以前的这个节点。&lt;/p&gt;
&lt;p&gt;我无限偏爱她抒情化的空镜头，泛出的柔光、环境音、易碎的颤动感。她镜头下的静物，和静止的相片相比，动人的正是时间穿行而过的窸窣感，柔光下的树木枝叶，风摇动的粼粼的光与阴影，触碰窗沿结着的雨珠，水的滴落，碗中浸泡的青豆浮动的光泽，晾晒的衣物被吹起的摆动，空转的圆盘形衣架。有某种薄如蝉翼的果冻一般的轻颤。&lt;/p&gt;
&lt;p&gt;被凝视着的安静的时间流淌，影像中的时间流速和现实有了同步的校正。就好像沙罗双树里俊骑单车载着夕现实里很短影像中又显得很长的那一段路。&lt;/p&gt;
&lt;p&gt;看了早期的短片，拥抱、在世界的沉默中、蜗牛、见到天堂，觉得自己可以一直一直看下去河濑直美的短片，一直一直平静下去，一直一直落入回忆的质感。电影里最喜欢的果然还是萌之朱雀和沙罗双树。也看了澄沙之味和光，故事却远远没有散景好。&lt;/p&gt;
&lt;h2 id=&#34;沙罗双树&#34;&gt;沙罗双树&lt;/h2&gt;
&lt;p&gt;喜欢这一段长长的骑行路。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776276.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776319.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776472.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776538.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776606.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;沙罗节，暴雨。日光和暴雨下的舞、喊词、两旁人群的加入、压抑许久释放出的笑。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776613.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;蜗牛&#34;&gt;蜗牛&lt;/h2&gt;
&lt;p&gt;代表理性旁观的摄影机镜头，代表个人情感的伸手触碰。&lt;/p&gt;
&lt;p&gt;印象深刻的是，摄像机一直正面面向祖母，靠近靠近又靠近，祖母笑了，说干嘛一直拍我，怎么不拍拍你自己。祖母笑着说，不要拍啦。祖母笑着说，我又不是很快就要死掉。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776964.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673776992.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777013.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777021.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777111.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777139.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;拥抱&#34;&gt;拥抱&lt;/h2&gt;
&lt;p&gt;我很喜欢日光过曝又暗下去的反复，像长时间安静站立在原地阳光被一片一片云遮挡又穿透而出，像记忆里背景光的质感。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777175.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777211.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777195.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777201.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777221.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777231.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777285.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;在世界的沉默中&#34;&gt;在世界的沉默中&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777303.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777360.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777310.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/01/upgit_20230115_1673777364.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
- https://bygonexf.github.io/post/kawase-naomi/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>布尔迪厄《继承人：大学生与文化》 笔记</title>
        <link>https://bygonexf.github.io/post/heriters/</link>
        <pubDate>Fri, 23 Dec 2022 15:04:21 +0800</pubDate>
        
        <guid>https://bygonexf.github.io/post/heriters/</guid>
        <description>Fxye. https://bygonexf.github.io/post/heriters/ -&lt;h2 id=&#34;我的废话&#34;&gt;我的废话&lt;/h2&gt;
&lt;p&gt;第一次读布尔迪厄。感觉自己高中阶段和父亲的争执、本科时期感受到的大学本身的矛盾性、关于小镇做题家的迷思，全都得到了解答。&lt;/p&gt;
&lt;h2 id=&#34;ch1-中选者的选择&#34;&gt;CH1 中选者的选择&lt;/h2&gt;
&lt;p&gt;对社会地位最低的人来说，接受高等教育的主观愿望比客观机会还要小。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;出身越高，选择分科时更遵循家庭建议。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;在对学习的态度方面，可以发现同样的差异。不管他们更赞同天才论，还是更相信自己的天才（二者并行不悖），&lt;strong&gt;出身于资产阶级的大学生在与他人一样承认存在脑力劳动技术的同时，又对此表示了更大的厌恶。这些技术，如拥有一套卡片或一个时间表，被公认为不能与智力冒险的浪漫形象并存&lt;/strong&gt;。甚至连学习爱好和过程方面的细微末节，也表现了出身于上层阶级的大学生对智育的无所谓态度。当他们对自己的爱好和能力更为肯定，用极为多样的文化兴趣表现出真正的或所谓的兴趣广泛和成果各异的业余爱好时，其他人则表现出对大学的更多依附。当问及社会学专业的大学生，吏喜欢研究自己的杜会、第三世界还是人种学时，人们发现，社会出身越高，选择“异国”题目和地点的人越多。同样，如果说出身高的大学生更喜欢时髦思想的话（比如认为“神话学”研究更好地体现了社会学的目标），&lt;strong&gt;是不是一直受保护的经历使他们的爱好更服从于娱乐的原则而不是实际的原则？是不是在智育方面追求异国情调和表面的好心，只是一种象征性的，或者说无足轻事的炫耀方式，他们借此在说明自己的资产阶级经历的时候把它摆脱掉？为了形成这样一种智力机制，是不是应当提供一一而且要在很长时间内 ― 自由和免费选择的经济与社会条件？&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;处于有利地位的大学生，不仅从其出身的环境中得到了习惯、训练、能力这些直接为他们学业服务的东西，而且也从那里&lt;strong&gt;继承了知识、技术和爱好&lt;/strong&gt;。一种“有益的爱好”对学习产生的间接效益，并不亚于前面那些因素。&lt;/p&gt;
&lt;p&gt;文化行为受到的社会因素的制约，大于个人的兴趣和爱好。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;可公共获得的领域，我们能收获相同的知识，然而态度和价值观依然会不同。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;纯学校文化，不只是不完全的文化或文化的一个组成部分，而且是一种低层次的文化。&lt;/p&gt;
&lt;p&gt;个人完成文化行为的方式赋予这些纯文化的性质：具有嘲弄味道的潇洒，故作风雅的简洁明了，使人态度自如或装成自如的合乎章法的自信，这些几乎总是发生在出身于上层阶级的大学生身上。在这些阶级里，上述行为方式标志着一个人属于精英之列。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;文化遗产以更隐蔽、更间接的方式传递，甚至不需要一步步的努力和明显的行动。可能正是最有“文化教养”的阶层，最不需要宣传对文化的崇拜或有意识地进行文化实践的启蒙。在小资产阶级里，情况正好相反。大部分情况下，家长除文化方面的良好愿望外不能传递别的什么东西。有文化教养的阶级却把冗长的教诲作了精心安排，以通过暗中说服的方式使后代接受文化。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;并且，反而学校会贬低自己传播的平民色彩的文化，理睬并助长了文化面前的最初不平等。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;可是，每个人都得参加这一游戏，因为它以具有普遍性价值的面目出现在人们面前&lt;/strong&gt;。如果说，出身于处在不利地位的阶级的儿童，经常发现上学就是学习那些人造的东西和教师的辞藻，&lt;strong&gt;那不正是因为对这些儿童来讲，学者式思考要先于直接经验吗&lt;/strong&gt;？他们必须详细地学习巴台农神庙的平面图，却从不离开自己居住的省份；他们必须在整个学习期间同样被迫地，而不是真心地，谈论那些他们也说不上来的东西，谈论过去人们所酷爱的见解肯定法，谈论爱好方面那些无限大和无限小的差异。反复讲传统教育排除了所教全部内容的真实性，就是回避这样一个事实：不同阶层大学生不真实的感觉十分不同。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此，从掌握文化的难易程度和愿望这两重意义上讲，工农出身的大学生处于最不利的地位：直到近一个时期，他们甚至还&lt;strong&gt;不能在家庭环境中找到吸引他们努力求学的因素&lt;/strong&gt;。正是这样的因素，使中产阶级通过&lt;!-- raw HTML omitted --&gt;对占有的渴望&lt;!-- raw HTML omitted --&gt;弥补了原来不占有这一不足。为了使一个儿童进入国立中学并在以后一帆风顺，必须有&lt;strong&gt;持续不断的成功&lt;/strong&gt;（和教师的频频告诫）。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;按照极不平等的严格程度选择出来的人面前的相对平等，可以掩盖作为它基础的不平等。&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;出身于中产阶级和文化教养式存在着细微的差别，他们的校内学习能不能取得同样大的成功？不能排除有的教师把”优秀的“或”天才的“学生与”认真的“学生对立起来，在很多情况下不考虑其他因素，只从学生出生时就注定了的与文化关系的角度来进行判断。中产阶级出身的大学生素来努力学习，并且在工作中发挥他们所处环境推崇的职业美德（如崇拜严格而艰难地完成了的工作）。很多教师在评价这些学生时，也有意识地使用对文化精英使用的标准，当这些学生进人”权威“行列从而属于”精英“时尤其如此。&lt;strong&gt;文化和脑力劳动的贵族形象与人们对文化的共同认识是如此地接近，致使它甚至影响到对精英理论最深信不疑的人，阻止他们在表面的平等之外再要求更多的东西&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;通过改变标记实现的价值观的颠倒，把认真变成了认真精神，把对劳动的崇尚变成了穷苦劳动者的斤斤计较和艰苦努力，并想以此来弥补天资的不足。从以”精英“的价值体系判断小资产阶级的价值体系的时候起，这一变化就开始了。就是说，按有教养、出身高的人的浅薄涉猎来衡量小资产阶级的价值体系。而这些人毫不费力地就掌握了知识，他们的现状和前途有保证，可以悠闲地追求风雅，敢于卖弄技巧。可是，精英文化与学校文化是如此地接近，小资产阶级出身的儿童（农民或工人的子弟更甚）只有十分刻苦，才能掌握教给有文化教养的阶级子弟的那些东西，如风格、兴趣、才智等。这些技能和礼仪是一个阶级所固有的，因为它们就是这个阶级的文化。&lt;strong&gt;对一些人来说，学到精英文化是用很大代价换来的成功；对另一些人来讲，这只是一种继承&lt;/strong&gt;，它同时包含着便当和便当的诱惑。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;如果推而广之，甚至可以说，&lt;strong&gt;这些社会决定因素的效果越不为人知，它们的决定作用就越无情&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;所以，除去把教育面前的所有不平等归咎于经济不平等或政治意图，从而向这一制度开战之外，没有为教育制度服务的更好方式。事实上，&lt;strong&gt;教育制度可以通过其自身逻辑的作用使特权永久化。换言之，就是它可以为特权服务，而不需特权人物主动利用&lt;/strong&gt;。此后，不管是对高等教育整体还是对它的一个方面而言，企图使教育制度的一个方面得以独立的要求，客观上服务于这一制度及其所效力的全部对象。因为，只需让这些因素从学前教育到高等教育发挥作用，就可以保证社会特权永久化。当奖学金或助学金制度表面上使出身于各个社会阶级的人在教育面前处于平等地位的时候，淘汰中下阶级儿童的机制就会发挥同样的作用（只是较为隐蔽）。这时候，人们可以比任何时候都更有理由，把不同社会阶层在不同层次的教育中所占比例的不平等归结于天资不同或愿望不同。&lt;/p&gt;
&lt;p&gt;总之，&lt;strong&gt;不平等的社会因素的作用巨大，它可以使教育制度在经济条件平等的情况下，把社会特权转化为天资或个人学习成绩，从而不中断地维护不平等。表面的机会均等实现得越好，学校就越可以使所有的合法外衣服务于特权的合法化&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;ch2-严肃游戏与游戏严肃&#34;&gt;CH2 严肃游戏与游戏严肃&lt;/h2&gt;
&lt;p&gt;无疑，哪里的大学生活发达，哪里就留下它的居住场所、活动空间和必经的路线。大学生居住和课余活动的场所尽管分散在城市各个地方，但仍有自己的特色。人们平常的称呼证明了这一点，如“大学生区”、“大学生”咖啡馆、“大学生”住房。大部分大学生只有上一样的课这一个共同点。除此之外，人们无法承认，共同生活和居住这个简单的事实，具有使它聚集的个体结合成一个协调的群体的能力：&lt;strong&gt;为一个群体提供一体化框架的不是空间，而是在时间中对空间的有规律和有节奏的使用&lt;/strong&gt;。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;可是，在法国大学的传统中，找不到任何对合作理想的支持。从小学到研究生阶段，以制度为基础的集体工作仅仅是极少数特例。在自己的任务中，教师们往往把组织功能排到最后，对组织大学生集体工作一事尤为如此。更有甚者，从童年时候起，学校反复向学生灌输的完全是一种相反的理想，那就是个人主义的竞争。所以，大学生们可以提出与大学本身格格不入的集体工作的愿望，但是在大学受的教育又没有为他们作任何准备，不能发明使自己与长期以来内化的价值观背道而驰的技术。在这种情况下，大学里的工作小组经常失败的原因，首先是由于大学生——这些由只发展被动倾向的系统制造出来的产品，不能靠决心产生奇迹，凭空创造出一体化的新形式。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;此种设计并不事先统一确定大学生象征性行为的内容，有时是专一地和有条理地把自己铸成大学生的意愿，不以一致承认理想大学生的形象为前提。因为，人们想实现的形象可以压缩为实现一种形象的迫切需要。想成为什么样的人和想自我选择，首先是拒绝成为不是自己选择的那种样子。在被拒绝或改变了的必然性当中，处在第一位的是社会出身。在避而不谈家长的职业方面，大学生们往往是一致的，而不管是什么职业。羞愧地沉默，说话半真半假，宣布与家庭断绝关系，都是与一种观点保持距离的好办法。这是学生们无法接受的观点：&lt;strong&gt;这种如此缺乏选择的决定，可以决定努力自我选择的人的一切&lt;/strong&gt;。自我塑造和自我选择的愿望不一定造成某种确定的行为，它只象征性地利用行为，证明自己已经选择了此种行为。所以，无论是大学生对大学生还是对自己个人的评论，无论结果是肯定的还是否定的，总是要回到他是什么这个造成他存在的问题上来。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;上面所说的肯定是一种极端现象。但是，是否可以说，具有这些传播手段的组织不可避免地传播一些东西，哪怕是它想传播并且认为正在传播的以外的东西？事实上，不管表面如何，&lt;strong&gt;大学一直是在说服那些已经被说服的人：既然它最终的使命是使人接受文化的价值，它就并非真正需要进行强制和惩罚，因为它的顾客都程度不同地向往进入知识分子阶层。可是，进人知识界只是有限的一部分大学生的合情合理的计划。那么，让所有的大学生，包括些以后不会进入知识界的大学生，都用几年的时间来经历这个假设的和游戏式的成为知识分子的过程，会有什么样的作用呢？&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;学习远不是一种简单的手段，它本身具有自己的目的。孤立地看待当前的学业，人们就会以对出发点和目的的的双重否定为代价，给自己一种全面体验知识分子志向的错觉。从此，学会在社会性决定因素面前做手脚和耍两面派，就成为一种很好的职业准备。因为这样可以掌握一些技术，知识分子通过这些技术可以获得真实的或虚构的对自由智力的体验。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;学校环境有不止一种特点使人想到赌博环境：规则的使用只是为了使人参加进来，时间和空间有限而且来自诸决定因素起作用的真实世界。这是因为，&lt;strong&gt;通过使人相信自己是赌注，学校比其他各种赌博都更强烈地希望或要求参加者对这一游戏更为依恋&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;大学最好的模式是合谋且虚构的对立，是自由辩论和自由命题论文。学校在压力之下，通过此类最为正式的练习，教授如何使用智力方面的自由。认为大学与大学文化有问题，不也是遵循了这一模式吗？因此，怎么能不看到，&lt;strong&gt;对学校制度的反抗和对异端的狂热追求，正在通过迂回的途径实现着大学所追求的最终目的&lt;/strong&gt;？就连那些最循规蹈矩的教师，尽管他们的本意不是这样，也会激发对一种被认为更富有生气、更真实的”反文化“的认同。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;表面最放荡不羁的行为，往往只是在传统模式的传统应用范围以外对这些模式的屈从&lt;/strong&gt;：逃学的好学生是文化游击队员。**如果美国西部片不是作为西文化出现，对西部片的狂热会和现在一样吗？**电影俱乐部负责人听到的议论和发言，是文学和哲学教授一直尽力启发而又往往得不到的。于是对外部强加的规则的反抗，是规则所强加的价值观得以内化的途径之一。这与弗洛伊德的想象有相似之处：被摄取的父亲是从他被杀的时候起才处于支配地位的。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此，巴黎大学生比其他所有的人都更倾向于&lt;strong&gt;把青年和社会的象征性隔绝与才智的形成混滑在一起&lt;/strong&gt; 。在这种情况下，尽管女大学生的很多选择受制于最传统的模式，她们当中不少人还是通过跨越性别标准在实现着自已所勾划的解放型女知识分子形象。此种解放的最高象征性结果，表现为它所批准的明确倒：先是推崇童贞·后来是另一种神话一一应当不惜一切地丢掉童贞。 而在一定程度上，&lt;strong&gt;某些政治归属的吸引力，往往就在于可以使人以最廉价又最严重的形式象征性地消受与家庭环境的隔绝&lt;/strong&gt;。&lt;strong&gt;与社会出身、职业前途及为之作准备的学业等所有约束保持距离，是典型的知识分子游戏，它呼呼并支持为拖饰而拖饰的游戏&lt;/strong&gt;。&lt;strong&gt;社会出身造成的差异越是被回避，人们在观点和爱好中强烈地表现出来的差异就越是明显和突出&lt;/strong&gt;。不同宗派以如此快的速度，以如此复杂的机制对抗、分化和组合的社会不是很多；论战游戏能以如此巨大的力量从中引起如此激情的群体也很少。于是，一个团体的少数人可以反对这个团体的多数人，并且并不因此而附和他们反对的人在里面处于少数的另一个更大团体中多数人的立场。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;事实上，寻找差异需要一个前提。那就是，&lt;strong&gt;在差异游戏的限制和在其中进行游戏的必要性两者之间，达成一&lt;/strong&gt;致。可是，不走出一致划定的范围，就很难发现真正的差异。这样，不同意见总是可能带有虚构性和表面性，人们所辩论的可能永远不是实质性问题。因为要想辩论，就要对实质问题取得一致。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;于是，他们的境遇所形成的表象，表现为他们境遇的反面。&lt;strong&gt;思想社会学通过将赞成与反对倒置，揭示了已经宣布的差异掩盖的一致和已经宣布的一致掩盖的差异&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677330282.png&#34; alt=&#34;image-20230225210434882&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ch3-学徒还是小巫&#34;&gt;CH3 学徒还是小巫&lt;/h2&gt;
&lt;p&gt;如果前途通过过多的中介与现实相联，那它总容易被认为是虚幻的。&lt;strong&gt;孤立地看这种本质上讲是暂时的和过渡的状态，就使大学生在忘记前途的同时忘记了自己&lt;/strong&gt;。为此目的，大学的传统向他们推荐了两大模式，&lt;strong&gt;一个是“考试能手”，一个是“浅薄涉猎”&lt;/strong&gt;。二者表面上矛盾，却都得到了赞同。前一种人被学习的成功所强烈吸引，除考试外忘掉了一切，出发点就是人们认为考试可以保证的资格。这种“走极端”的大学生的视野受到只注意学习报应的局限，他们与只知道智力探险的无限遥远前景的“浅薄涉猎”者表面上正好相反。认为学艺本身就是自的的幻想，产生了去作知识分子——永恒的学徒的吸引力。但这只是神话般地进行着，因为必须要否认学艺真正为之服务的的，即进入一种职业——知识分子。在上面两种情况下，不论是使现时永恒化还是使它孤立化，所作的努力同样都是使客观地呼唤自己消失的现时固定不变。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;被迫面对一个比较现实主义的职业计划，下层阶级出身的大学生永远不能完全堕入浅薄涉猎之中，或者被学习暂时的魅力所吸引。对他们来说，学习首先是一个机会，一个应当抓住的在社会等级中得以迁升的机会。必要性就是法律。他们更了解也更接受自己正在为之准备的职业，更清楚也更承认自己正在为一种职业做准备。&lt;strong&gt;大学生与他们的前途，即与他们的学业的关系，直接与本阶级的人接受高等教育的客观机会有关&lt;/strong&gt;。上层阶级的大学生可以满足于空泛的计划，因为他们从未必须真正选择他们所作的事，这在他们的阶级甚至家庭中司空见惯。可是，下层阶级出身的大学生不能不对自己正在做的事提出疑问，因为他们忘记自己原本可以不做这件事的机会要少得多。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;因此，如果说在大学生界经常见到的职业“技术”几乎总是具有魔法的性质，这绝非偶然。无疑，&lt;strong&gt;现行制度的逻辑是鼓励学生的惰性和依附性，使他们处在一个不能完全受完全合理的方式所支配的境地&lt;/strong&gt;：比如，通过贬低成功秘方的作用，通过有时去掩盖正在发挥吸引力（有时是其全部吸引力）的物质和智力技术，通过使他们的判断标准含混不清，&lt;strong&gt;具有天赐能力的教师只能加深大学生的无能感、武断敢和注定失败感&lt;/strong&gt;。在大学生这方面，因为&lt;strong&gt;他们更喜欢这样，因为相信天赐的能力比相信通过艰苦劳动去掌握技术所付出的代价要低&lt;/strong&gt;，所以他们就在缺乏天资的情况下，认为只有魔法才可以对学业的成功产生作用。&lt;/p&gt;
&lt;h2 id=&#34;ch4-结论&#34;&gt;CH4 结论&lt;/h2&gt;
&lt;p&gt;这样，学校的具有合法化作用的权威可以加重社会方面的不平等。因为，&lt;strong&gt;处于最不利地位的阶级对自己的命运过于觉悟，对于实现命运的途径又过于不觉悟，从而促进了自己命运的实现&lt;/strong&gt;。&lt;/p&gt;
- https://bygonexf.github.io/post/heriters/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>耶，VAE</title>
        <link>https://bygonexf.github.io/post/vae/</link>
        <pubDate>Sun, 10 Jul 2022 23:43:59 +0000</pubDate>
        
        <guid>https://bygonexf.github.io/post/vae/</guid>
        <description>Fxye. https://bygonexf.github.io/post/vae/ -&lt;h1 id=&#34;引子&#34;&gt;引子&lt;/h1&gt;
&lt;h2 id=&#34;生成模型&#34;&gt;生成模型&lt;/h2&gt;
&lt;p&gt;能从可学习的概率分布中采样得到样本的模型。&lt;/p&gt;
&lt;p&gt;在一些生成模型中，样本通过将随机的隐层变量送入网络生成得到。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234631635.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;自编码器-ae&#34;&gt;自编码器 AE&lt;/h2&gt;
&lt;p&gt;自编码器通过学习从输入到隐层和从隐层到输出的映射来重建信号/图像。&lt;/p&gt;
&lt;p&gt;目标：$X&amp;rsquo; = D_\theta(E_\phi(X)) \approx X$&lt;/p&gt;
&lt;p&gt;$\mathop{min}\limits_{\theta, \phi} \sum\limits_{i=1}^n||D_\theta(E_\phi(X_i))-X_i||^2$，其中 ${{X_i}}_{i=1\cdots n}$ 为数据集。&lt;/p&gt;
&lt;p&gt;自编码器并不是一种生成模型，因为它并没有定义一个概率分布，无法采样。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自编码器 → 生成模型？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们会有一个很自然的做生成模型的想法，那就是训练一个从低维隐层变量生成观测样本的生成模型，最大化观测数据似然。&lt;/p&gt;
&lt;p&gt;假设这个生成模型为 $G_{\theta}:\mathbb{R}^k \rightarrow \mathbb{R}^d$，其中 $k &amp;lt; d$，将隐层变量 $Z$ 映射为样本 $X$，那么其实在样本空间里几乎大部分区域 $p(X)=0$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234646078.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;如果我们从样本空间看，在这个高维空间只会有非常小的一个低维空间子集 $p(X)$ 是有值的，并且我们在训练的时候其实是不知道这个子集的分布的，而其余大部分区域 $p(X)=0$，也就意味着我们很难直接优化似然。&lt;/p&gt;
&lt;p&gt;但是有一种方法可以让我们在每一处都得到非零值，那就是在已有先验 $p(Z)$ 的条件下，定义一个有噪声的观测模型 $p_\theta(X|Z)=\mathcal{N}(X;G_\theta(Z), \eta I)$ (其中 $\eta$ 是可调整的参数，$I$ 是单位矩阵)。&lt;/p&gt;
&lt;p&gt;所以 $p(X) = \int p(Z)p(X|Z)\mathrm{d}Z$，这个值也很难去计算，所以我们不是去优化 $p(X)$ 而是去优化 $p(X)$ 的下限（变分推断里的证据下限 ELBO，后面会证明）。&lt;/p&gt;
&lt;p&gt;那么这其实就是 VAE 的雏形了。&lt;/p&gt;
&lt;h1 id=&#34;变分自编码器-vae&#34;&gt;变分自编码器 VAE&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234659598.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;上面的图是 VAE 的整体思路，生成的部分也是也就是 decoder 的部分，我们会假设 $Z$ 服从一个简单的先验分布 $p(Z)$，这个分布可以是一个标准正态分布。通过 decoder 会得到高维图像空间的&lt;strong&gt;一个概率分布&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而 encoder 端，注意 VAE 有一个很重要的想法是，我们不去直接计算难以计算的 $p_\theta(Z|X)$，而是用另外单独学习的网络去模拟一个 $q_\phi(Z|X)$，它其实是 $p_\theta(Z|X)$ 的一个近似。&lt;/p&gt;
&lt;h2 id=&#34;p_thetax-推导&#34;&gt;$p_\theta(X)$ 推导&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234713512.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再看一下 $p_\theta(X)$  下界的推导过程。①：因为 $p_\theta(X)$ 是独立于 $Z$ 的，所以我们可以去计算它在 $Z$ 上的期望；②：条件概率公式；③：上下约一个 $q_\phi(Z|X)$；④：右边的项其实就是 $q_\phi(Z|X)$ 和 $p_\theta(Z|X)$ 的 KL 散度，KL 散度衡量的是两个概率分布之间的相似性，两者差异越小，KL 散度越小，两分布完全一致时 KL 散度才为 0，所以因为右项恒大于等于 0，我们可以把左项视为 $log\space p_\theta(X)$ 的下界。之后就不直接优化 $log\space p_\theta(X)$，而是优化这个下界。&lt;/p&gt;
&lt;h2 id=&#34;variational-lower-bound-的直观解释&#34;&gt;Variational Lower Bound 的直观解释&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234726362.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再来看一下怎么理解这个变分下界，注意我们的目标是最大化这个下界。首先用条件概率公式替换一下，之后把式子拆成两部分，下面来解释一下为什么第一项是重建误差，第二项是正则项。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234737121.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;之前说了我们建立了一个有噪声的观测模型 $p_\theta(X|Z)=\mathcal{N}(X;G_\theta(Z), \eta I)$ (就是 $Z$ 通过 decoder 后得到的那个高维图像空间的分布)，正态分布的公式不用说了吧，代入一下就会发现第一项是一个 L2 距离，最大化这个负的 L2 距离就是在减小重建误差，encourage $q_\phi$ to be point mass，这句话我是这么理解的，point mass 其实是离散的概率分布，减小重建误差就是在消除我们加的这个高斯噪声，让它成为类似于我们最先讲的那个被舍弃的点到点的离散概率模型（但注意它又是连续概率，所以就是接近奇异分布？）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234745973.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再看第二项，这一项可以写成一个 KL 散度，我们最大化负的这个 KL 散度就是在让 $q_\phi(Z|X)$ 和 $p(Z)$ 两个概率分布尽可能接近。上一项重建损失是鼓励 $q_\phi$ 去成为 point mass，这里则是平滑 $q_\phi$ 去使它尽可能接近标准正态分布。&lt;/p&gt;
&lt;p&gt;可以看到两项之间存在相驳的张力，前一项试图让 $q_\phi$ 成为奇异分布，后一项则试图让 $q_\phi$ 不要成为奇异分布。可以理解为前者鼓励它准，后者鼓励它具有更强的生成性。&lt;/p&gt;
&lt;h2 id=&#34;vae-架构&#34;&gt;VAE 架构&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727234755246.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;再来看一下 VAE 的实际架构。主要有两点值得细说。&lt;/p&gt;
&lt;p&gt;第一点是，如果我们对每个 $X_i$ 找最最佳的 $q_\phi(Z|X_i)$，然后优化 $\phi$，这样的更新代价会很大。所以我们不这么做，而是去学习一个 inference 网络预测这个 $q_\phi(Z|X_i)$ 的均值和方差（实际上预测的是  $\mu$ 和 $log \space \sigma$），这样 inference 阶段的模型参数对于所有的数据参数是共享的，就可以分摊学习和更新的成本。&lt;/p&gt;
&lt;p&gt;第二点是，采样的操作本身是不能反向传播的，所以采样这里用到了重参数化的技巧，也就是从 $\mathcal{N}(0,1)$ 中采样一个 $\varepsilon$，然后让 $Z=\mu + \varepsilon \times \sigma$，这样采样的操作就可以独立于网络之外，其他所有环节都能进行反向传播。&lt;/p&gt;
&lt;h2 id=&#34;stochastic-gradient-optimization-of-vlb&#34;&gt;Stochastic Gradient Optimization of VLB&lt;/h2&gt;
&lt;p&gt;Todo…&lt;/p&gt;
&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://khoury.northeastern.edu/home/hand/teaching/cs7150-summer-2020/Variational_Autoencoders.pdf&#34;&gt;https://khoury.northeastern.edu/home/hand/teaching/cs7150-summer-2020/Variational_Autoencoders.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=c27SHdQr4lw&#34;&gt;https://www.youtube.com/watch?v=c27SHdQr4lw&lt;/a&gt; （力荐 👍）&lt;/p&gt;
- https://bygonexf.github.io/post/vae/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>端到端图像/视频压缩里的熵模型</title>
        <link>https://bygonexf.github.io/post/e2e-entropy-model/</link>
        <pubDate>Wed, 20 Apr 2022 17:04:37 +0800</pubDate>
        
        <guid>https://bygonexf.github.io/post/e2e-entropy-model/</guid>
        <description>Fxye. https://bygonexf.github.io/post/e2e-entropy-model/ -&lt;h2 id=&#34;概率分布与熵编码&#34;&gt;概率分布与熵编码&lt;/h2&gt;
&lt;p&gt;在端到端图像/视频压缩模型中，我们需要去尽可能精准地模拟待编码元素值的概率分布。一方面是为了更精确地进行码率估计，另一方面也是因为更精准的概率分布建模能使得熵编码环节更好地消除统计冗余节省码字。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230224_1677253790.png&#34; alt=&#34;image-20230224234946517&#34;&gt;&lt;/p&gt;
&lt;p&gt;建模出概率分布后，在实际熵编码中，就可以通过概率分布生成熵编码器所需要的概率表。&lt;/p&gt;
&lt;p&gt;多说一句，在传统编解码里，通常熵编码会采用自适应模型，即随着编码字符的输入，不断更新概率分布（自适应模型相比静态模型效率更高，符合局部性原理，适应符号概率忽大忽小的波动，如果能合理地利用上下文信息压缩效率可以远超静态模型）。然而在端到端压缩模型里，通常直接通过网络生成独立的概率分布的参数，不会随着编码过程更新概率表。&lt;/p&gt;
&lt;h2 id=&#34;量化不可导&#34;&gt;量化不可导&lt;/h2&gt;
&lt;p&gt;这个没什么好说的，量化四舍五入的取整操作显然是不可导的，所以在训练的时候可以通过加均匀噪声来替换四舍五入的操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677299134.png&#34; alt=&#34;image-20230225122531069&#34;&gt;&lt;/p&gt;
&lt;p&gt;在训练阶段，我们会通过给待编码元素值加上-0.5到0.5的均匀噪声来替代量化操作。而实际推理的时候，就正常进行量化。&lt;/p&gt;
&lt;h2 id=&#34;均匀噪声涉及到的概率关系&#34;&gt;均匀噪声涉及到的概率关系&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677304856.png&#34; alt=&#34;image-20230225124013214&#34;&gt;&lt;/p&gt;
&lt;p&gt;首先明确一下这几条线分布代表什么。&lt;/p&gt;
&lt;p&gt;$p_{y_i}$：编码空间元素值的概率密度函数&lt;/p&gt;
&lt;p&gt;$p_{\tilde{y_i}}$：$y_i$ 加上均匀噪声后的概率密度函数&lt;/p&gt;
&lt;p&gt;$p_{\hat{y_i}}$：$y_i$量化后的概率质量函数（量化后就成了离散型变量了）&lt;/p&gt;
&lt;p&gt;均匀噪声其实就是均匀分布 $U(-0.5, 0.5)$，$y_i$ 加上均匀噪声得到 $\tilde{y_i}$，两个独立的连续随机变量的和的概率分布公式是 $f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x) ,\mathrm{d}x$，直观来说也很好理解，对于任意 $\tilde{y_i}$ 值为 $c$，可能加均匀噪声得到 $c$ 的 $y_i$ 取值范围其实就是 $c-0.5$ 到 $c+0.5$，$p_{\tilde{y_i}}$ 在 $c$ 点的值其实就可以通过 $p_{y_i}$ 在 $c-0.5$ 到 $c+0.5$ 的积分得出。&lt;/p&gt;
&lt;p&gt;对于每个整数点，也自然符合上述描述。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677304320.png&#34; alt=&#34;image-20230225135158633&#34;&gt;&lt;/p&gt;
&lt;p&gt;而这样一来，加均匀噪声得到的 $p_{\tilde{y_i}}$ 最妙的性质就在于，在每个整数点 $p_{\tilde{y_i}}$ 的值和实际量化得到的离散变量 $p_{\hat{y_i}}$ 在这一点的概率质量相等。&lt;/p&gt;
&lt;p&gt;所以说，加均匀噪声这一操作，本质上类似于在给 $p_{\hat{y_i}}$ 的概率质量函数作插值，类似于一个连续松弛 (continuous relaxation) 的操作。&lt;/p&gt;
&lt;p&gt;此外，我们在端到端模型里通常去建模的也就是这个 $p_{y_i}$，而这里其实是假设 $p_{y_i}$ 近似一个拉普拉斯分布，实际代码实现中，有一部分模型采用拉普拉斯分布去建模，也有一部分模型，比如 CompressAI，是采用高斯分布去建模的。&lt;/p&gt;
&lt;h2 id=&#34;compressai-代码中的熵模型&#34;&gt;CompressAI 代码中的熵模型&lt;/h2&gt;
&lt;p&gt;以其中的 GaussianConditional 熵模型为例，稍微讲一下实际实现的时候一些常见操作。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;forward&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        inputs: Tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        scales: Tensor,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        means: Optional[Tensor] = &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        training: Optional[&lt;span style=&#34;color:#658b00&#34;&gt;bool&lt;/span&gt;] = &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ) -&amp;gt; Tuple[Tensor, Tensor]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; training &lt;span style=&#34;color:#8b008b&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            training = self.training
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        outputs = self.quantize(inputs, &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;noise&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; training &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;else&lt;/span&gt; &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;dequantize&amp;#34;&lt;/span&gt;, means)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        likelihood = self._likelihood(outputs, scales, means)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; self.use_likelihood_bound:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            likelihood = self.likelihood_lower_bound(likelihood)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; outputs, likelihood
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;训练的时候和上面说的一样，通过加均匀噪声替代量化操作。&lt;/p&gt;
&lt;p&gt;而其中这个 &lt;code&gt;likelihood&lt;/code&gt; 其实就是用于码率估计的，网络会输出 $y_i$ 值，然后我们叠加均匀噪声，而网络也会输出 $p_{y_i}$ 建模为高斯分布的 $\mu$ 和 $\sigma$ 值，这样其实我们就能计算出当前条件下 $p_{\tilde{y_i}}$ 的特定取值。后面再用 $-log_2 x$ 就能算出估计出的码字比特大小。&lt;/p&gt;
&lt;p&gt;再来看一下实际熵编解码的实现，以编码为例。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;compress&lt;/span&gt;(self, inputs, indexes, means=&lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;        Compress input tensors to char strings.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;        Args:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;            inputs (torch.Tensor): input tensors
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;            indexes (torch.IntTensor): tensors CDF indexes
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;            means (torch.Tensor, optional): optional tensor means
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#cd5555&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        symbols = self.quantize(inputs, &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;symbols&amp;#34;&lt;/span&gt;, means)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#658b00&#34;&gt;len&lt;/span&gt;(inputs.size()) &amp;lt; &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#008b45;font-weight:bold&#34;&gt;ValueError&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;Invalid `inputs` size. Expected a tensor with at least 2 dimensions.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;if&lt;/span&gt; inputs.size() != indexes.size():
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;raise&lt;/span&gt; &lt;span style=&#34;color:#008b45;font-weight:bold&#34;&gt;ValueError&lt;/span&gt;(&lt;span style=&#34;color:#cd5555&#34;&gt;&amp;#34;`inputs` and `indexes` should have the same size.&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._check_cdf_size()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._check_cdf_length()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._check_offsets_size()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        strings = []
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#8b008b&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#658b00&#34;&gt;range&lt;/span&gt;(symbols.size(&lt;span style=&#34;color:#b452cd&#34;&gt;0&lt;/span&gt;)):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            rv = self.entropy_coder.encode_with_indexes(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                symbols[i].reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                indexes[i].reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self._quantized_cdf.tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self._cdf_length.reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                self._offset.reshape(-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int().tolist(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            strings.append(rv)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; strings
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;symbols&lt;/code&gt; 就是量化后的待编码值减去网络预测出的高斯分布的均值，这样后面熵编码就可以统一用准备好的不同方差的零均值高斯采样的 cdf 表。再注意一下这里的 &lt;code&gt;indexes&lt;/code&gt; ，下文会讲。&lt;/p&gt;
&lt;p&gt;上面说的 cdf 表可以通过这个更新函数看一下实现过程。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;update&lt;/span&gt;(self):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        multiplier = -self._standardized_quantile(self.tail_mass / &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pmf_center = torch.ceil(self.scale_table * multiplier).int()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pmf_length = &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt; * pmf_center + &lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        max_length = torch.max(pmf_length).item()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        device = pmf_center.device
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples = torch.abs(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            torch.arange(max_length, device=device).int() - pmf_center[:, &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;None&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples_scale = self.scale_table.unsqueeze(&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples = samples.float()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        samples_scale = samples_scale.float()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        upper = self._standardized_cumulative((&lt;span style=&#34;color:#b452cd&#34;&gt;0.5&lt;/span&gt; - samples) / samples_scale)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        lower = self._standardized_cumulative((-&lt;span style=&#34;color:#b452cd&#34;&gt;0.5&lt;/span&gt; - samples) / samples_scale)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        pmf = upper - lower
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        tail_mass = &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt; * lower[:, :&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        quantized_cdf = torch.Tensor(&lt;span style=&#34;color:#658b00&#34;&gt;len&lt;/span&gt;(pmf_length), max_length + &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        quantized_cdf = self._pmf_to_cdf(pmf, tail_mass, pmf_length, max_length)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._quantized_cdf = quantized_cdf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._offset = -pmf_center
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        self._cdf_length = pmf_length + &lt;span style=&#34;color:#b452cd&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到，cdf 表其实是一定数量采样的 scale 值对应的零均值高斯分布（转化后的 $p_{y_i}$）在一定数量采样点上计算好 $p_{\tilde{y_i}}$ 的值。&lt;/p&gt;
&lt;p&gt;这里其实有两处采样：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;采样 scale （对应 index ，index 指明取 cdf 表哪个分布）&lt;/li&gt;
&lt;li&gt;对于分布采样一系列的点计算概率值 （对应 cdf 表中每个分布）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可以通过 &lt;code&gt;build_indexes&lt;/code&gt; 来看一下 indexes 的确定过程。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;build_indexes&lt;/span&gt;(self, scales: Tensor) -&amp;gt; Tensor:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        scales = self.lower_bound_scale(scales)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        indexes = scales.new_full(scales.size(), &lt;span style=&#34;color:#658b00&#34;&gt;len&lt;/span&gt;(self.scale_table) - &lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;).int()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;for&lt;/span&gt; s &lt;span style=&#34;color:#8b008b&#34;&gt;in&lt;/span&gt; self.scale_table[:-&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;]:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            indexes -= (scales &amp;lt;= s).int()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; indexes
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;indexes&lt;/code&gt; 是通过 &lt;code&gt;scales&lt;/code&gt; 来确定的，具体来说 &lt;code&gt;index&lt;/code&gt; 其实是之前说的一系列采样的 scale 值里小于等于当前 scale 的最接近它的采样 scale 的编号。&lt;/p&gt;
- https://bygonexf.github.io/post/e2e-entropy-model/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>可变形卷积与光流</title>
        <link>https://bygonexf.github.io/post/dcn-and-optical-flow/</link>
        <pubDate>Sun, 20 Feb 2022 17:04:37 +0800</pubDate>
        
        <guid>https://bygonexf.github.io/post/dcn-and-optical-flow/</guid>
        <description>Fxye. https://bygonexf.github.io/post/dcn-and-optical-flow/ -&lt;h2 id=&#34;可变形卷积代码篇&#34;&gt;可变形卷积代码篇&lt;/h2&gt;
&lt;p&gt;一个调用 &lt;code&gt;from mmcv.ops import ModulatedDeformConv2d&lt;/code&gt; 的例子：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#eed;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#008b45&#34;&gt;forward&lt;/span&gt;(self, &lt;span style=&#34;color:#658b00&#34;&gt;input&lt;/span&gt;):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        x = self.offset_mask_conv(&lt;span style=&#34;color:#658b00&#34;&gt;input&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        o1, o2, mask = torch.chunk(x, &lt;span style=&#34;color:#b452cd&#34;&gt;3&lt;/span&gt;, dim=&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        offset = torch.cat((o1, o2), dim=&lt;span style=&#34;color:#b452cd&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        mask = torch.sigmoid(mask)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        output = self.dcnv2(&lt;span style=&#34;color:#658b00&#34;&gt;input&lt;/span&gt;, offset, mask)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#8b008b;font-weight:bold&#34;&gt;return&lt;/span&gt; output
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;总而言之 &lt;code&gt;offset&lt;/code&gt; 的 size 就是 &lt;code&gt;2*kernel[0]*kernel[1]&lt;/code&gt; ，想一想，原来我们求偏移的时候，会把 &lt;code&gt;B*H*W*C&lt;/code&gt; 的图像送入普通卷积得到 &lt;code&gt;B*H*W*2C&lt;/code&gt; 得到偏移，也就是每个通道每个位置点都有 x 和 y 两个方向的偏移量。&lt;/p&gt;
&lt;p&gt;对 DCN 来说，每个通道都做一样的处理，也就是只需要对每个位置点存卷积核每个点的 x 和 y 的偏移，所以就是 &lt;code&gt;B*H*W*(2*kernel_size)&lt;/code&gt; 。&lt;/p&gt;
&lt;p&gt;关于 mask:：置信 mask，并非必需，不作展开了&lt;/p&gt;
&lt;p&gt;所以 offset 和 mask 一起就是 &lt;code&gt;B*H*W*(3*kernel_size)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;关于 deform_groups：本来是所有通道公用，也可以改成划成几组，组数就是 deform_groups，那这样就是 &lt;code&gt;B*H*W*(group_num*3*kernel_size)&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;可变形卷积与光流&#34;&gt;可变形卷积与光流&lt;/h2&gt;
&lt;p&gt;端到端视频压缩模型里，运动估计运动补偿环节，常用到光流和可变形卷积。本质上都是预测偏移，只不过 DCN 可以利用多个偏移，这样在预测难度较大的位置就有多个 offset 互作补充，所以比只用单一的光流更有优势一些。&lt;/p&gt;
&lt;p&gt;推荐一下这篇文章：&lt;a href=&#34;https://ckkelvinchan.github.io/projects/DCN/&#34;&gt;Understanding Deformable Alignment in Video Super-Resolution&lt;/a&gt;，讲得比较透彻了。&lt;/p&gt;
&lt;h3 id=&#34;可变形卷积涉及多少个不同的-offset-map&#34;&gt;可变形卷积涉及多少个不同的 offset map&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先，与 kernel size 有关&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677309897.png&#34; alt=&#34;image-20230225152456053&#34;&gt;&lt;/p&gt;
&lt;p&gt;假设特征图大小为 &lt;code&gt;W * H&lt;/code&gt;，每个点的感受野都对应了 &lt;code&gt;kernel_size * kernel_size&lt;/code&gt; 个偏移&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其次，与 group num 有关&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677310198.png&#34; alt=&#34;image-20230225152648179&#34;&gt;&lt;/p&gt;
&lt;p&gt;每 &lt;code&gt;C(通道数) / group_num&lt;/code&gt; 个通道共用一套 offset&lt;/p&gt;
&lt;p&gt;比如假设特征有 8 个通道，group_num 为 4,则 每 2 个通道共用一套偏移参数&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;综合来说，一共会有 &lt;code&gt;(kernel_size * kernel_size) * group_num&lt;/code&gt; 个 &lt;code&gt;W * H&lt;/code&gt; 大小的偏移图。&lt;/p&gt;
&lt;h3 id=&#34;dcn-对齐和光流对齐的本质差异&#34;&gt;DCN 对齐和光流对齐的本质差异&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/2023/02/upgit_20230225_1677310203.png&#34; alt=&#34;image-20230225152953943&#34;&gt;&lt;/p&gt;
&lt;p&gt;$kernel \space size 为 n * n的DCN = n^2个空间warping \space +\space 1 * 1 * n^2 的3D卷积$&lt;/p&gt;
&lt;p&gt;也就是说，如果 n=1，group_num=1，DCN对齐基本就等同于光流对齐，DCN 和光流的本质区别就在于 offset diversity。&lt;/p&gt;
- https://bygonexf.github.io/post/dcn-and-optical-flow/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>一些传统的熵编码方法</title>
        <link>https://bygonexf.github.io/post/entropy-coding/</link>
        <pubDate>Sun, 05 Dec 2021 13:09:24 +0000</pubDate>
        
        <guid>https://bygonexf.github.io/post/entropy-coding/</guid>
        <description>Fxye. https://bygonexf.github.io/post/entropy-coding/ -&lt;h1 id=&#34;传统熵模型&#34;&gt;传统熵模型&lt;/h1&gt;
&lt;h2 id=&#34;算术编码-arithmetic-coding&#34;&gt;算术编码 (Arithmetic Coding)&lt;/h2&gt;
&lt;h3 id=&#34;流程&#34;&gt;流程&lt;/h3&gt;
&lt;p&gt;（以静态模型举例）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;假设有一段数据需要编码，统计里面所有的字符和出现的次数。编码从初始区间 (0, 1] 开始。&lt;/li&gt;
&lt;li&gt;在当前区间内根据各字符概率划分子区间。&lt;/li&gt;
&lt;li&gt;读入字符，找到该字符落入的子区间，将区间更新为该子区间，并重复 2, 3 步骤&lt;/li&gt;
&lt;li&gt;最后得到的区间 [low, high) 中任意一个小数以二进制形式输出即得到编码的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例子如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727131100497.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;实现细节&#34;&gt;实现细节&lt;/h3&gt;
&lt;p&gt;最后结果是一个小数，我们不能简单地用一个 double 类型去表示和计算这个小数，因为根据数据的复杂程度，这个小数可能任意长，小数点后可能会有成千上万位。&lt;/p&gt;
&lt;p&gt;然而，小数点后的数据前几位很有可能是在过程中是可以不断提前确定的。例如如果当前区间为 [0.14432, 0.1456)，高位的 0.14 可以提前确定，14已经可以输出了。那么小数点可以向后移动两位，区间变成 [0.432, 0.56)，在此基础上进行后面的计算。这样编码区间永远保持在一个有限的精度要求上。&lt;/p&gt;
&lt;p&gt;上述是基于十进制的，实际数字是用二进制表示的，当然原理是一样的，用十进制只是为了表述方便。&lt;/p&gt;
&lt;h3 id=&#34;静态模型--自适应模型&#34;&gt;静态模型 → 自适应模型&lt;/h3&gt;
&lt;p&gt;静态模型在初始时对完整的数据统计完概率分布，之后不再更新概率分布；自适应模型随着字符的输入会不断更新概率分布。&lt;/p&gt;
&lt;p&gt;静态模型的缺点在于：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在压缩前对信息内字符进行统计的过程会消耗大量时间。&lt;/li&gt;
&lt;li&gt;对较长的信息，静态模型统计出的符号概率是该符号在整个信息中的出现概率，而自适应模型可以统计出某个符号在某一局部的出现概率或某个符号相对于某一上下文的出现概率，换句话说，自适应模型得到的概率分布将有利于对信息的压缩（可以说结合上下文的自适应模型的信息熵建立在更高的概率层次上，其总熵值更小），好的基于上下文的自适应模型得到的压缩结果将远远超过静态模型。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;例如一段码流，某符号在前面出现概率较大而后面概率小，甚至忽大忽小，采用自适应模型就可以更好的适应这样的变动，压缩效率会比静态模型更高。主流视频编码标准如H.264/H.265都使用自适应模型。&lt;/p&gt;
&lt;h3 id=&#34;算术编码-vs-哈夫曼编码&#34;&gt;算术编码 vs 哈夫曼编码&lt;/h3&gt;
&lt;p&gt;首先说结论，算术编码压缩效率更高，哈夫曼编码复杂度更低。&lt;/p&gt;
&lt;p&gt;这两种编码，或者说熵编码的本质是，概率越小的字符，用更多的 bit 去表示，这反映到概率区间上就是，概率小的字符所对应的区间也小，因此这个区间的上下边际值的差值越小，为了唯一确定当前这个区间，则需要更多的数字去表示它。&lt;/p&gt;
&lt;p&gt;哈夫曼编码由于不断地二叉，它的子区间总是 $\frac{1}{2}$ 的幂次方。而算术编码可以做到严格按照概率的大小等比例划分子区间。所以哈夫曼编码只是算术编码一种粗略的近似。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727131117834.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;cabac&#34;&gt;CABAC&lt;/h3&gt;
&lt;p&gt;CABAC（Context-based Adaptive Binary Arithmetic Coding），CABAC 被视频标准H.264/H.265所采用。&lt;/p&gt;
&lt;p&gt;CABAC可以分为二值化、上下文建模和二进制算术编码三个步骤。&lt;/p&gt;
&lt;p&gt;其中上下文建模相当于把整段码流进行了再次的细分，把相同条件下的字符bin（比如块大小/亮度色度/语法元素/扫描方式/周围情况等）归属于某个context，形成一个比较独立的子队列而进行编码，其更新只与当前的状态和当前字符是否MPS有关（换句话说，只和历史该子队列编码字符和当前字符有关），而与别的子队列/字符是无关的。当然输出码字往往是根据规则而“混”在一起的。&lt;/p&gt;
&lt;p&gt;CABAC虽然性能很好，但也存在以下几点不足：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;复杂度过高，不易并行处理。存在块级依赖（左/上角的块没有码率估计/熵编码，后继块就无法得到更新后的状态，从而无法开始码率估计/熵编码）、Bin级依赖（同一个子队列的bin存在前后依赖性，后继的bin要等前面bin编完后才能得到更新后的上下文状态）以及编码的几个环节依赖，这些依赖性会影响编码器的并行实现。&lt;/li&gt;
&lt;li&gt;计算精度问题。为简化计算，CABAC采用128个状态来近似，根据原来状态和当前符号性质查表得到下个状态。这个过程中会有一些精度的损失。另外，如果当一连串的MPS到来，状态到达62后就不会继续改变，只会“原地踏步”。换句话说，当概率到达0.01975时就不会随着符号继续变小，这样会影响压缩效率。&lt;/li&gt;
&lt;li&gt;Context的设计问题。部分context利用频率很低，在测试中平均一帧都用不到几次，而有的context使用频率很高，需要进一步的优化。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;区间编码-range-coding&#34;&gt;区间编码 (Range Coding)&lt;/h2&gt;
&lt;p&gt;区间编码可以看为算术编码的一个变种，比算术编码压缩效率略小，但运算速度近乎是算术编码的两倍。&lt;/p&gt;
&lt;p&gt;区间编码在整数（任意底）空间中进行进行计算，而算术编码的区间总是以小数的形式进行迭代。其他部分都几乎一样。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220727131132728.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;端到端熵模型&#34;&gt;端到端熵模型&lt;/h1&gt;
&lt;p&gt;Todo&amp;hellip;&lt;/p&gt;
&lt;h1 id=&#34;参考&#34;&gt;参考&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/23834589&#34;&gt;算术编码（转载加笔记）&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000011561822&#34;&gt;算数编码原理解析&lt;/a&gt;&lt;/p&gt;
- https://bygonexf.github.io/post/entropy-coding/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>图像质量评价指标(MSE, PSNR, MS-SSIM)</title>
        <link>https://bygonexf.github.io/post/image-quality-evaluation-metrics/</link>
        <pubDate>Thu, 02 Dec 2021 23:35:47 +0000</pubDate>
        
        <guid>https://bygonexf.github.io/post/image-quality-evaluation-metrics/</guid>
        <description>Fxye. https://bygonexf.github.io/post/image-quality-evaluation-metrics/ -&lt;p&gt;如何评价重建图像的质量：比较重建图像与原始图像的可视误差。&lt;/p&gt;
&lt;h2 id=&#34;mse&#34;&gt;MSE&lt;/h2&gt;
&lt;p&gt;Mean Squared Error, 均方误差&lt;/p&gt;
&lt;p&gt;$MSE = \frac{1}{N}\sum\limits_{i=1}^{N}(x_i-y_i)^2$&lt;/p&gt;
&lt;p&gt;两者越接近，MSE 越小。MSE 损失的范围为 0 到 ∞。&lt;/p&gt;
&lt;h2 id=&#34;psnr&#34;&gt;PSNR&lt;/h2&gt;
&lt;p&gt;Peak Signal to Noise Ratio，峰值信噪比，即峰值信号的能量与噪声的平均能量之比，通常取 log 单位为分贝。&lt;/p&gt;
&lt;p&gt;$PSNR = 10 log_{10}\frac{MaxValue^2}{MSE}$&lt;/p&gt;
&lt;p&gt;从式子可以看出 PSNR 可以理解为 MSE 的另一种表达形式。与 MSE 相反的是，重建图像质量越好，PSNR 数值越大。&lt;/p&gt;
&lt;p&gt;对于图像来说，像素点数值以量化方式保存，八比特位深的情况，取值范围为 [0, 255]，$MaxValue$ 就是 255。&lt;/p&gt;
&lt;h2 id=&#34;ssim&#34;&gt;SSIM&lt;/h2&gt;
&lt;p&gt;MSE 与 PSNR 的问题是，在计算每个位置上的像素差异时，其结果仅与当前位置的两个像素值有关，与其它任何位置上的像素无关。这种计算差异的方式仅仅将图像看成了一个个孤立的像素点，而忽略了图像内容所包含的一些视觉特征，特别是图像的局部结构信息。而图像质量的好坏极大程度上是一个主观感受，其中结构信息对人主观感受的影响非常之大。&lt;/p&gt;
&lt;p&gt;而 SSIM (Structural Similarity，结构相似性) 就试图解决这个问题&lt;/p&gt;
&lt;p&gt;SSIM 由三部分组成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;亮度对比 平均灰度作为亮度测量： $\mu_x = \frac{1}{N}\sum\limits_{i=1}^{N}x_i$ 亮度对比函数： $l(x,y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2+\mu_y^2+C_1}$&lt;/li&gt;
&lt;li&gt;对比度对比 灰度标准差作为对比度测量： $\sigma_x={(\frac{1}{N-1}\sum\limits_{i=1}^N{(x_i-\mu_x)}^2)}^{\frac{1}{2}}$ 亮度对比函数： $c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}$&lt;/li&gt;
&lt;li&gt;结构对比 结构测量： $\frac{x-\mu_x}{\sigma_x}$ 结构对比函数： $s(x,y) = \frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y + C_3}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;SSIM 函数：&lt;/p&gt;
&lt;p&gt;$SSIM(x,y)={[l(x,y)]}^\alpha \cdot {[c(x,y)]}^\beta \cdot {[s(x,y)]}^\gamma$&lt;/p&gt;
&lt;p&gt;$一般取 \alpha = \beta =\gamma=1$&lt;/p&gt;
&lt;p&gt;$SSIM(x,y)=\frac{(2\mu_x\mu_y+C_1)(2\sigma_x\sigma_y+C_2)}{(\mu_x^2+\mu_y^2+C_1)(\sigma_x^2\sigma_y^2+C_2)}$&lt;/p&gt;
&lt;p&gt;下图是同样 MSE 的图片，仅仅做对比拉伸（灰度拉伸，增大图像灰度级的动态范围）、均值偏移，其实不怎么影响人眼对图像的理解，而模糊和压缩痕迹则影响较大，这些情况下 SSIM 就能更好地做出判断。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220723233900866.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ms-ssim&#34;&gt;MS-SSIM&lt;/h2&gt;
&lt;p&gt;SSIM 算法基于 HVS 擅长从图像中提取结构信息，并利用结构相似度计算图像的感知质量。但 SSIM 是一种单尺度算法，实际上正确的图像尺度取决于用户的观看条件，如显示设备分辨率、用户的观看距离等。&lt;/p&gt;
&lt;p&gt;单尺度的 SSIM 算法可能仅适用于某个特定的配置，为了解决该问题，MS-SSIM (Multi-scale structural similarity) 在 SSIM 算法的基础上提出了多尺度的结构相似性评估算法。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/image-20220723233921947.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;MS-SSIM 算法，L 表示低通滤波器，2↓ 表示采样间隔为 2 的下采样&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;原始图像的尺度为 1，最大尺度为 M，对 $scale=j$ 的尺度而言，其亮度、对比度、结构的相似性分布表示为 $l_j(x,y), c_j(x,y), s_j(x,y)$，MS-SSIM 的计算公式为：&lt;/p&gt;
&lt;p&gt;$MS-SSIM(x,y) = {[l_M(x,y)]}^{\alpha M} \cdot \prod\limits_{j=1}^M{[c_j(x,y)]}^{\beta j}{[s_j(x,y)]}^{\gamma j}$&lt;/p&gt;
&lt;p&gt;一般，令 $\alpha_j = \beta_j = \gamma_j$，$j \in [1, M]$，我们得到：&lt;/p&gt;
&lt;p&gt;$MS-SSIM(x,y) = {[l_M(x,y)]}^{\alpha M} \cdot \prod\limits_{j=1}^M{[c_j(x,y) \cdot s_j(x,y)]}^{\alpha j}$&lt;/p&gt;
- https://bygonexf.github.io/post/image-quality-evaluation-metrics/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>AVS3 编码位流</title>
        <link>https://bygonexf.github.io/post/avs3-bitstream/</link>
        <pubDate>Thu, 01 Apr 2021 17:28:29 +0000</pubDate>
        
        <guid>https://bygonexf.github.io/post/avs3-bitstream/</guid>
        <description>Fxye. https://bygonexf.github.io/post/avs3-bitstream/ -&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;视频序列&lt;/strong&gt;是位流的最高层语法结构。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;帧&lt;/strong&gt;由一个亮度样本矩阵和两个色度样本矩阵构成。&lt;strong&gt;场&lt;/strong&gt;由构成帧的三个样本矩阵中相间的行构成。奇数行构成顶场，偶数行构成底场。&lt;/p&gt;
&lt;p&gt;视频序列头由视频序列起码码开始，后面跟着一串编码图像数据。序列头可在位流中重复出现，称为重复序列头。使用重复序列头的主要目的是支持对视频序列的随机访问。&lt;/p&gt;
&lt;p&gt;一副图像可以是一帧或一场，其编码数据由图像起始码开始，到序列起始码、序列结束码或下一个图像起始码结束。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;片&lt;/strong&gt;是图像中的矩形区域，包含若干最大编码单元在图像内的部分，片之间不应重叠。&lt;/p&gt;
&lt;p&gt;图像划分为&lt;strong&gt;最大编码单元&lt;/strong&gt;，最大编码单元之间不应重叠，最大编码单元左上角的样本不应超出图像边界，最大编码单元右下角的样本可超出图像边界。&lt;/p&gt;
&lt;p&gt;最大编码单元划分为一个或多个&lt;strong&gt;编码单元&lt;/strong&gt;，由编码树决定。编码单元划分为一个或多个&lt;strong&gt;变换块&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;码流结构及语义描述&#34;&gt;码流结构及语义描述&lt;/h2&gt;
&lt;h3 id=&#34;视频序列&#34;&gt;视频序列&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210403170108.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;序列头 sequence_header&lt;/p&gt;
&lt;p&gt;视频序列起始码、档次标号、级别标号、知识位流标志、知识图像允许标志、知识位流重复序列头标志、逐行序列标志、场图像序列标志、水平尺寸、垂直尺寸、色度格式、样本精度、编码样本精度、宽高比、帧率代码、比特率低位、比特率高位、低延迟、时间层标识允许标志、位流缓冲区尺寸、最大解码图像缓冲区大小、参考图像队列 1 索引存在标志、参考图像队列相同标志、参考图像队列配置集数、默认活跃参考图像数、最大编码单元尺寸、最小编码单元尺寸、划分单元最大比例、编码树最大划分次数、最小四叉树尺寸、最大二叉树尺寸、最大扩展四叉树尺寸、加权量化允许标志、加权量化矩阵加载标志、二次变换允许标志、样值偏倚补偿允许标志、自适应修正滤波允许标志、仿射运动补偿允许标志、对称运动矢量差模式允许标志、脉冲编码调制模式允许标志、自适应运动矢量精度允许标志、候选历史运动信息数、帧内预测滤波允许标志、高级运动矢量表达模式允许标志、运动矢量精度扩展模式允许标志、色度两步预测模式允许标志、帧内衍生模式允许标志、衍生模式划分边长最大尺寸、基于位置的变换允许标志、图像重排序延迟、跨片环路滤波允许标志、片划分一致性标志、参考同位置片标志、统一片大小标志、片宽度、片高度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;视频编辑码和视频序列结束码&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;视频编辑码 video_edit_code&lt;/p&gt;
&lt;p&gt;紧跟其后的第一幅 I 图像后续的 B 图像可能因缺少参考图像而不能正确解码&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;视频序列结束码 video_sequence_end_code&lt;/p&gt;
&lt;p&gt;标识视频序列的结束。如果 POI（显示顺序索引），如果 POI 的值大于 $(2^{32}-1)$，位流中应插入一个视频序列结束码。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;参考图像队列配置集&lt;/p&gt;
&lt;p&gt;参考知识图像标志、知识图像索引标志、被参考的知识图像索引、参考图像数、参考图像 DOI 差值绝对值、参考图像 DOI 差值符号&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自定义加权量化矩阵&lt;/p&gt;
&lt;p&gt;加权量化矩阵系数&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;图像&#34;&gt;图像&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;帧内预测图像头&lt;/p&gt;
&lt;p&gt;帧内预测图像起始码、BBV 延时、时间编码标志、时间编码、解码顺序索引、知识图像索引、时间层标识、图像输出延迟、引用参考图像队列配置集标志、引用参考图像队列配置集索引、BBV 检测次数、逐行帧标志、图像编码结构标志、顶场在先、重复首场、顶场场图像标志、固定图像量化因子、去块滤波禁用标志、去块滤波参数标志、$\alpha$ 和 C 索引的偏移、$\beta$ 索引的偏移、色度量化参数禁用标志、色度量化参数增量 Cb、色度量化参数增量 Cr、图像加权量化允许标志、图像加权量化数据加载索引、加权量化参数索引、加权量化矩阵模型、加权量化参数增量 1、加权量化参数增量 2、图像自适应修正滤波允许标志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;帧间预测图像头&lt;/p&gt;
&lt;p&gt;帧间预测图像起始码、随机访问正确解码标志、图像编码方式、活跃参考图像数重载标志、活跃参考图像数、仿射预测子块尺寸标志&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;片&lt;/p&gt;
&lt;p&gt;片起始码、固定片量化银子标志、片量化因子、片样值偏移补偿允许标志、高级熵编码字节对齐填充位、最大编码单元量化参数增量、样值偏移补偿合并方式索引、样值偏移补偿模式、样值偏移补偿区间模式偏移绝对值、样值偏移补偿区间模式偏移值符号值、样值偏移补偿区间模式起始偏移子区间位置、样值偏移补偿区间模式起始偏移子区间位置差、样值偏移补偿模式偏移值、样值偏移补偿边缘模式类型、最大编码单元自适应修正滤波允许标志、熵编码最大编码单元填充位、片结束码&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;编码树&#34;&gt;编码树&lt;/h3&gt;
&lt;p&gt;四叉树划分标志、编码单元预测模式、二叉树扩展四叉树划分标志、二叉树扩展四叉树划分类型标志、二叉树扩展四叉树划分方向标志&lt;/p&gt;
&lt;h3 id=&#34;编码单元&#34;&gt;编码单元&lt;/h3&gt;
&lt;p&gt;跳过模式标志、高级运动矢量表达模式标志、仿射模式标志、直接模式标志、帧内编码单元标志、基础运动矢量索引、运动矢量偏移量索引、运动矢量方向索引、仿射运动矢量索引、衍生模式划分标志、衍生模式划分方向、水平四叉衍生模式划分标志、垂直四叉衍生模式划分标志、水平非对称衍生模式标志、仿射自适应运动矢量精度索引、自适应运动矢量精度索引、编码单元子类型索引、预测参考模式、对称运动矢量差标志、运动矢量精度扩展模式标识、帧内亮度预测模式、帧内色度预测模式、帧内预测滤波标志、L0 预测单元参考索引、L0 运动矢量水平分量差绝对值、L0 运动矢量垂直分量差绝对值、L0 运动矢量水平分量差符号值、L0 运动矢量垂直分量差符号值、仿射帧间模式L0 运动矢量水平分量差绝对值、仿射帧间模式 L0 运动矢量垂直分量差绝对值、仿射帧间模式 L0 运动矢量水平分量差符号值、仿射帧间模式 L0 运动矢量垂直分量差符号值、L1&amp;hellip;、变换块系数标志、基于位置的变换块标志、Cb 变换块编码模板、Cr 变换块编码模板、亮度变换块编码模板&lt;/p&gt;
- https://bygonexf.github.io/post/avs3-bitstream/ - Faye</description>
        </item>
    
    
    
        <item>
        <title>H.265/HEVC 预测编码 笔记</title>
        <link>https://bygonexf.github.io/post/prediction-coding/</link>
        <pubDate>Wed, 31 Mar 2021 17:04:49 +0000</pubDate>
        
        <guid>https://bygonexf.github.io/post/prediction-coding/</guid>
        <description>Fxye. https://bygonexf.github.io/post/prediction-coding/ -&lt;h2 id=&#34;视频预测编码技术&#34;&gt;视频预测编码技术&lt;/h2&gt;
&lt;p&gt;预测编码是指利用已编码的一个或几个样本值，根据某种模型或方法，对当前的样本值进行预测，并对样本真实值和预测值之间的差值进行编码。&lt;/p&gt;
&lt;h3 id=&#34;帧内预测编码&#34;&gt;帧内预测编码&lt;/h3&gt;
&lt;p&gt;随着离散余弦变换 (DCT) 在图像、视频编码中的广泛应用，帧内预测转为在频域进行，如相邻块 DC 系数的差分编码等。由 DCT 的性质可知，DC 系数仅能反映当前块像素值的平均大小，因此上述频域中基于 DC 系数的帧内预测无法反映出视频的纹理信息，这限制了频域帧内预测的发展。&lt;/p&gt;
&lt;p&gt;H.264/AVC 标准中使用基于块的空域帧内预测方法，规定了若干种预测模式，每种模式都对应一种纹理方向（DC 模式除外），当前块预测像素由其预测方向上相邻块的边界重建像素生成。该方法使得编码器能根据视频内容特征自适应地选择预测模式。&lt;/p&gt;
&lt;p&gt;H.264/AVC 使用拉格朗日率失真优化 (RDO) 进行模式选择。它为每一种模式计算其拉格朗日代价：
$$
J = D + \lambda \cdot R
$$
其中，$D$ 表示当前预测模式下地失真，$R$ 表示编码当前预测模式下所有信息（如变换系数、模式细腻些、宏块划分方式等）所需的比特数。&lt;strong&gt;最优的预测模式不一定满足残差最小，而应指残差信号经过其他编码模块后最终的编码性能最优。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;H.264/AVC 标准及后来的 FRExt 扩展层一共规定了 3 种大小的亮度帧内预测块：4 × 4、8 × 8 以及 16 × 16。其中 4 × 4 和 8 × 8 块包含 9 种预测模式，16 × 16 块包含 4 种预测模式。色度分量的帧内预测都是基于 8 × 8 大小的块进行的，也有 4 种预测模式。&lt;/p&gt;
&lt;h3 id=&#34;帧间预测编码&#34;&gt;帧间预测编码&lt;/h3&gt;
&lt;h4 id=&#34;帧间预测编码原理&#34;&gt;帧间预测编码原理&lt;/h4&gt;
&lt;p&gt;目前主要的视频编码标准帧间预测部分都采样了基于块的运动补偿技术。其主要原理是为当前图像的每个像素块在之前已编码图像中寻找一个最佳匹配块，该过程被称为&lt;strong&gt;运动估计 (Motion Estimation, ME)&lt;/strong&gt;。其中被参考的图像称为&lt;strong&gt;参考图像 (Reference Frame)&lt;/strong&gt;，参考块到当前像素块的位移称为&lt;strong&gt;运动向量 (Motion Vector, MV)&lt;/strong&gt;，当前像素块与参考块的差值称为&lt;strong&gt;预测残差 (Prediction Residual)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;在 H.261 标准中，P 图像的预测方式只有前向预测。但实际场景中往往会产生不可预测的运动和遮挡，因此当前图像可能在之后的图像中更容易找到匹配块。为此，MPEG-1 标准定义了第三类图像，B 图像。为了提高运动估计精度，MPEG-1 首次采用了半像素精度的运动估计，半像素位置的参考像素值可由双线性差值方法产生。&lt;/p&gt;
&lt;p&gt;面向数字广播电视的标准 MPEG-2 首次支持了隔行扫描视频。一帧图像包含两个场，顶场和底场，每个帧图像的宏块需要被拆分成两个 16 × 8 的块分别进行预测。H.263 标准沿用了 MPEG-1 的双向预测和半像素精度运动估计，并进一步发展了 MPEG-2 中将一个宏块分成更小的块进行预测的思想。&lt;/p&gt;
&lt;p&gt;H.264/AVC 标准规定了 7 种大小的运动补偿块，一个宏块内部允许存在不同大小块的组合。此外 H.264/AVC 还使用了 1/4 精度像素运动估计、多参考图像预测、加权预测以及空域/时域 MV 预测等。&lt;/p&gt;
&lt;h4 id=&#34;帧间预测编码关键技术&#34;&gt;帧间预测编码关键技术&lt;/h4&gt;
&lt;h5 id=&#34;1-运动估计&#34;&gt;1. 运动估计&lt;/h5&gt;
&lt;p&gt;在大多数视频序列中，相邻图像内容非常相似，不需要对每幅图像的全部信息编码，只需要将当前图像中运动物体的运动信息传给解码器。运动估计就是提取当前图像运动信息的过程。&lt;/p&gt;
&lt;p&gt;将图像分为不同大小的像素块，只要块大小选择合适，则各个块的运动形式可以看成是统一的，每个块的运动参数可以独立地估计，这就是常用地基于块地运动表示法。&lt;/p&gt;
&lt;p&gt;有几个核心问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;运动估计准则&lt;/p&gt;
&lt;p&gt;常用地匹配准则主要有最小均方误差 (MSE)、最小平均绝对误差 (MAD) 和最大匹配像素数 (MPC) 等。为了简化计算，一般使用绝对误差和 (SAD) 来代替 MAD。此外，最小变换域绝对误差和 (SATD) 也是一种性能优异的匹配准则。&lt;/p&gt;
&lt;p&gt;最小 SAD 准则不含乘除法，且便于硬件实现，因而使用最广泛。SAD 准则仅考虑了残差的大小，而未考虑编码运动信息所需的比特数。因此，H.264/AVC 编码器在运动估计过程中使用 RDO 来选择 MV。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;搜索算法&lt;/p&gt;
&lt;p&gt;常用的搜索算法有全搜索算法、二维对数搜索算法、三步搜索算法等。除全搜索算法，其余算法统称为快速算法。快速算法容易陷入局部最优点，为避免这一点，在搜索算法的每一步中尽量搜索更多的点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;亚像素精度运动估计&lt;/p&gt;
&lt;p&gt;亚像素精度运动估计意味着需要对参考图像进行插值，好的插值方法能大幅改善运动补偿的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;2-mv-预测&#34;&gt;2. MV 预测&lt;/h5&gt;
&lt;p&gt;在大多数图像和视频中，一个运动物体可能会覆盖多个运动补偿块，因此空间域相邻块的运动向量具有较强的相关性。若使用相邻已编码块对当前块 MV 预测，将二者差值进行编码，则会大幅减少编码 MV 所需的比特数。同时，由于物体运动具有连续性，因此相邻图像同一位置像素块的 MV 也具有一定相关性。H.264/AVC 使用了空域和时域两种 MV 的预测方式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;MV 空域预测&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/NMJ$SDFJ_%60NZL~05%7DP~67IA.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331212942.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MV 时域预测&lt;/p&gt;
&lt;p&gt;在 H.264/AVC 中，MV 时域预测主要针对 B Slice。主要有以下两种形式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当 B 图像的两个 MV 都来自同一个方向时（都来自当前图像之前的参考图像或之后的），其中一个 MV 可用另一个 MV 来预测&lt;/p&gt;
&lt;p&gt;设两参考图像 $ref_0$ 和 $ref_1$ 与当前图像的距离分别为 $l_0$ 和 $l_1$，二者 MV 分别为 $MV_0$ 和 $MV_1$，则 $MV_1$ 可由下式预测：
$$
MVP_1 = \frac{l_1}{l_0} MV_0
$$&lt;/p&gt;
&lt;p&gt;$$
MVD_1 = MV_1 - MVP_1
$$&lt;/p&gt;
&lt;p&gt;编码器只需要传输 $MVD_1$，解码器可按相同规则产生 $MV_1$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;直接模式 MV 预测&lt;/p&gt;
&lt;p&gt;H.264/AVC 为 B Slice 提供一种 Direct Mode。在该模式下，MV 可直接预测的出，无需传送 MV 差值。预测方式有时域空域两种。时域预测介绍如下。&lt;/p&gt;
&lt;p&gt;设两参考图像 $ref_0$ 和 $ref_1$ 分别位于当前图像的前方和后方，与当前图像的距离分别为 $l_0$ 和 $l_1$，且 $ref_1$ 中与当前块对应位置块有一个指向 $ref_0$ 的 MV，则当前图像的两个 MV 可计算如下：
$$
MV_0 = \frac{l_0}{l_0 + l_1}MV
$$
$$
MV_1 = MV_0 - MV
$$&lt;/p&gt;
&lt;p&gt;MV 时域预测主要运用了自然界物体匀速运动的思想。&lt;/p&gt;
&lt;p&gt;与 H.264 标准相比，H.265 剔除里 Merge 和 AMVP 两种先进的 MV 预测技术。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多参考图像及加权预测&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;帧内预测&#34;&gt;帧内预测&lt;/h2&gt;
&lt;h3 id=&#34;帧内预测模式&#34;&gt;帧内预测模式&lt;/h3&gt;
&lt;h4 id=&#34;亮度帧内预测模式&#34;&gt;亮度帧内预测模式&lt;/h4&gt;
&lt;p&gt;H.265/HEVC 亮度分量帧内预测支持 5 种大小的 PU，每一种大小的 PU 都对应 35 种预测模式，包括 Planar 模式、DC 模式以及 33 种角度模式。所有预测模式都使用相同的模板。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/%5D5%25KJ%7BTWLY0XF107LI%7DGWLD.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Planar 模式&lt;/p&gt;
&lt;p&gt;由 H.264/AVC 中的 Plane 模式发展而来，适用于像素值缓慢变化的区域。使用水平和垂直方向两个线性滤波器，并将二者的平均值作为当前块像素的预测值。这一做法能使预测像素平缓变化，与其他模式相比能提升视频的主观质量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DC 模式&lt;/p&gt;
&lt;p&gt;适用于大面积平坦区域。当前块预测值可由其左侧和上方（不包含左上角、左下方和右上方）参考像素的平均值得到。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;角度模式&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;亮度模式的编码&#34;&gt;亮度模式的编码&lt;/h4&gt;
&lt;p&gt;H.265/HEVC 标准建立了一个帧内预测模式候选列表 candModeList，表中有 3 个候选预测模式，用于存储相邻 PU 的预测模式。&lt;/p&gt;
&lt;h4 id=&#34;色度模式的编码&#34;&gt;色度模式的编码&lt;/h4&gt;
&lt;p&gt;共有 5 种模式：Planar 模式、垂直模式、水平模式、DC 模式以及对应亮度分量的预测模式。若对应亮度预测模式为前四种之一，则替换为角度预测中的模式 34。&lt;/p&gt;
&lt;h3 id=&#34;帧内预测过程&#34;&gt;帧内预测过程&lt;/h3&gt;
&lt;p&gt;在 H.265/HEVC 中，35 种预测模式是在 PU 的基础上定义的，而具体帧内预测过程的实现则是以 TU 为单位的。标准规定 PU 可以以四叉树的形式划分 TU，且一个 PU 内的所有 TU 共享一种预测模式。&lt;/p&gt;
&lt;p&gt;帧内预测可分为以下 3 个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断当前 TU 相邻参考像素是否可用（边界或未编码的就不可用）并作相应处理&lt;/li&gt;
&lt;li&gt;对参考像素进行滤波&lt;/li&gt;
&lt;li&gt;根据滤波后的参考像素计算当前 TU 的预测像素值&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;帧间预测&#34;&gt;帧间预测&lt;/h2&gt;
&lt;h3 id=&#34;运动估计&#34;&gt;运动估计&lt;/h3&gt;
&lt;h4 id=&#34;搜索算法&#34;&gt;搜索算法&lt;/h4&gt;
&lt;p&gt;在基于块运动补偿的视频编码框架中，运动搜索是最为重要的环节之一，也是编码器最耗时的模块。H.265/HEVC 官方测试编码器 HM10.0 给出了两种搜索算法：全搜索算法和 TZSearch 算法。&lt;/p&gt;
&lt;h4 id=&#34;亚像素精度运动估计&#34;&gt;亚像素精度运动估计&lt;/h4&gt;
&lt;h3 id=&#34;mv-预测技术&#34;&gt;MV 预测技术&lt;/h3&gt;
&lt;p&gt;H.265/HEVC 在 MV 预测方面提出了两种新技术，Merge 技术和 AMVP 技术。二者区别主要体现于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Merge 可以看成一种编码模式，在该模式下，当前 PU 的 MV 直接由空域或时域上邻近的 PU 预测得到，不存在 MVD (MV Difference)；而 AMVP 可以看成一种 MV 预测技术，编码器只需要对实际 MV 与预测 MV的差值进行编码，因此存在 MVD。&lt;/li&gt;
&lt;li&gt;二者 MV 候选列表长度不同，构建候选 MV 列表的方式也有所区别&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;merge-模式&#34;&gt;Merge 模式&lt;/h4&gt;
&lt;p&gt;为当前 PU 建立一个 MV 候选列表，列表中存在 5 个候选 MV（及其对应的参考图像），通过遍历这 5 个候选 MV，并进行率失真代价的计算，选取率失真代价最小的一个作为该 Merge 模式的最优 MV。若编/解码端按相同的方式键立该候选列表，则编码器只需要传输最优 MV 在候选列表中的索引即可。&lt;/p&gt;
&lt;p&gt;Merge 模式建立的 MV 候选列表包含时域和空域两种情形，对于 B Slice，还包含组合列表的方式。&lt;/p&gt;
&lt;h5 id=&#34;空域候选列表的建立&#34;&gt;空域候选列表的建立&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/NMJ$SDFJ_%60NZL~05%7DP~67IA.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;时域候选列表的建立&#34;&gt;时域候选列表的建立&lt;/h5&gt;
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bygonexf/Blog-Images/master/20210331212942.pn&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h5 id=&#34;组合列表的建立&#34;&gt;组合列表的建立&lt;/h5&gt;
&lt;p&gt;对于 B Slice 中的 PU 而言，由于存在两个 MV，因此其 MV 候选列表也需要提供两个预测 MV。H.265/HEVC 将 MV 候选列表中的前 4 个 MV 进行两两组合，产生了用于 B Slice 的组合列表。&lt;/p&gt;
&lt;h4 id=&#34;amvp-技术&#34;&gt;AMVP 技术&lt;/h4&gt;
&lt;p&gt;高级运动向量预测 (Advanced Motion Vector Prediction, AMVP) 为当前 PU 建立候选 MV 列表，编码器从中最优的预测 MV，并对 MV 进行差分编码；解码端通过建立相同的列表，仅需要将 MVD 与预测 MV 在该列表中的序号即可计算出当前 PU 的 MV。&lt;/p&gt;
&lt;p&gt;类似于 Merge 模式，AMVP 候选 MV 列表也包含空域和时域两种情形，不同的是 AMVP 列表长度仅为 2。&lt;/p&gt;
&lt;h3 id=&#34;加权预测&#34;&gt;加权预测&lt;/h3&gt;
&lt;p&gt;加权预测可用于修正 P Slice 或 B Slice 中的运动补偿预测像素。H.265/HEVC 中介绍了两种加权预测方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;默认加权预测&lt;/p&gt;
&lt;p&gt;未使用权值 $\omega$，根据参考图像队列的不同分 3 种情况计算。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explicit 加权预测&lt;/p&gt;
&lt;p&gt;其权值 $\omega$ 由编码器决定，并需要传送至解码端。也分 3 种情况。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;pcm-模式&#34;&gt;PCM 模式&lt;/h2&gt;
&lt;p&gt;在 PCM 模式下，编码器直接传输一个 CU 的像素值，而不经过预测、变换等其他操作。&lt;/p&gt;
&lt;p&gt;对于一些特殊情况，例如当图像的内容极不规则或量化参数非常小时，该模式与传统的“帧内 - 变换 - 量化 - 熵编码”相比，效率会更高。此外，PCM 模式还适用于无损编码情形。&lt;/p&gt;
- https://bygonexf.github.io/post/prediction-coding/ - Faye</description>
        </item>
    
    
  </channel>
</rss> 