<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>图像编码 on 好雪片片</title>
    <link>https://imfaye.me/tags/%E5%9B%BE%E5%83%8F%E7%BC%96%E7%A0%81/</link>
    <description>Recent content in 图像编码 on 好雪片片</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Faye</copyright>
    <lastBuildDate>Wed, 20 Apr 2022 17:04:37 +0800</lastBuildDate><atom:link href="https://imfaye.me/tags/%E5%9B%BE%E5%83%8F%E7%BC%96%E7%A0%81/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>端到端图像/视频压缩里的熵模型</title>
      <link>https://imfaye.me/post/e2e-entropy-model/</link>
      <pubDate>Wed, 20 Apr 2022 17:04:37 +0800</pubDate>
      
      <guid>https://imfaye.me/post/e2e-entropy-model/</guid>
      <description>概率分布与熵编码 在端到端图像/视频压缩模型中，我们需要去尽可能精准地模拟待编码元素值的概率分布。一方面是为了更精确地进行码率估计，另一方面也是因为更精准的概率分布建模能使得熵编码环节更好地消除统计冗余节省码字。
建模出概率分布后，在实际熵编码中，就可以通过概率分布生成熵编码器所需要的概率表。
多说一句，在传统编解码里，通常熵编码会采用自适应模型，即随着编码字符的输入，不断更新概率分布（自适应模型相比静态模型效率更高，符合局部性原理，适应符号概率忽大忽小的波动，如果能合理地利用上下文信息压缩效率可以远超静态模型）。然而在端到端压缩模型里，通常直接通过网络生成独立的概率分布的参数，不会随着编码过程更新概率表。
量化不可导 这个没什么好说的，量化四舍五入的取整操作显然是不可导的，所以在训练的时候可以通过加均匀噪声来替换四舍五入的操作。
在训练阶段，我们会通过给待编码元素值加上-0.5到0.5的均匀噪声来替代量化操作。而实际推理的时候，就正常进行量化。
均匀噪声涉及到的概率关系 首先明确一下这几条线分布代表什么。
$p_{y_i}$：编码空间元素值的概率密度函数
$p_{\tilde{y_i}}$：$y_i$ 加上均匀噪声后的概率密度函数
$p_{\hat{y_i}}$：$y_i$量化后的概率质量函数（量化后就成了离散型变量了）
均匀噪声其实就是均匀分布 $U(-0.5, 0.5)$，$y_i$ 加上均匀噪声得到 $\tilde{y_i}$，两个独立的连续随机变量的和的概率分布公式是 $f_{X+Y}(z) = \int_{-\infty}^{\infty} f_X(x) f_Y(z-x) ,\mathrm{d}x$，直观来说也很好理解，对于任意 $\tilde{y_i}$ 值为 $c$，可能加均匀噪声得到 $c$ 的 $y_i$ 取值范围其实就是 $c-0.5$ 到 $c+0.5$，$p_{\tilde{y_i}}$ 在 $c$ 点的值其实就可以通过 $p_{y_i}$ 在 $c-0.5$ 到 $c+0.5$ 的积分得出。
对于每个整数点，也自然符合上述描述。
而这样一来，加均匀噪声得到的 $p_{\tilde{y_i}}$ 最妙的性质就在于，在每个整数点 $p_{\tilde{y_i}}$ 的值和实际量化得到的离散变量 $p_{\hat{y_i}}$ 在这一点的概率质量相等。
所以说，加均匀噪声这一操作，本质上类似于在给 $p_{\hat{y_i}}$ 的概率质量函数作插值，类似于一个连续松弛 (continuous relaxation) 的操作。
此外，我们在端到端模型里通常去建模的也就是这个 $p_{y_i}$，而这里其实是假设 $p_{y_i}$ 近似一个拉普拉斯分布，实际代码实现中，有一部分模型采用拉普拉斯分布去建模，也有一部分模型，比如 CompressAI，是采用高斯分布去建模的。
CompressAI 代码中的熵模型 以其中的 GaussianConditional 熵模型为例，稍微讲一下实际实现的时候一些常见操作。
def forward( self, inputs: Tensor, scales: Tensor, means: Optional[Tensor] = None, training: Optional[bool] = None, ) -&amp;gt; Tuple[Tensor, Tensor]: if training is None: training = self.</description>
    </item>
    
    <item>
      <title>一些传统的熵编码方法</title>
      <link>https://imfaye.me/post/entropy-coding/</link>
      <pubDate>Sun, 05 Dec 2021 13:09:24 +0000</pubDate>
      
      <guid>https://imfaye.me/post/entropy-coding/</guid>
      <description>传统熵模型 算术编码 (Arithmetic Coding) 流程 （以静态模型举例）
假设有一段数据需要编码，统计里面所有的字符和出现的次数。编码从初始区间 (0, 1] 开始。 在当前区间内根据各字符概率划分子区间。 读入字符，找到该字符落入的子区间，将区间更新为该子区间，并重复 2, 3 步骤 最后得到的区间 [low, high) 中任意一个小数以二进制形式输出即得到编码的数据 例子如下：
实现细节 最后结果是一个小数，我们不能简单地用一个 double 类型去表示和计算这个小数，因为根据数据的复杂程度，这个小数可能任意长，小数点后可能会有成千上万位。
然而，小数点后的数据前几位很有可能是在过程中是可以不断提前确定的。例如如果当前区间为 [0.14432, 0.1456)，高位的 0.14 可以提前确定，14已经可以输出了。那么小数点可以向后移动两位，区间变成 [0.432, 0.56)，在此基础上进行后面的计算。这样编码区间永远保持在一个有限的精度要求上。
上述是基于十进制的，实际数字是用二进制表示的，当然原理是一样的，用十进制只是为了表述方便。
静态模型 → 自适应模型 静态模型在初始时对完整的数据统计完概率分布，之后不再更新概率分布；自适应模型随着字符的输入会不断更新概率分布。
静态模型的缺点在于：
在压缩前对信息内字符进行统计的过程会消耗大量时间。 对较长的信息，静态模型统计出的符号概率是该符号在整个信息中的出现概率，而自适应模型可以统计出某个符号在某一局部的出现概率或某个符号相对于某一上下文的出现概率，换句话说，自适应模型得到的概率分布将有利于对信息的压缩（可以说结合上下文的自适应模型的信息熵建立在更高的概率层次上，其总熵值更小），好的基于上下文的自适应模型得到的压缩结果将远远超过静态模型。 例如一段码流，某符号在前面出现概率较大而后面概率小，甚至忽大忽小，采用自适应模型就可以更好的适应这样的变动，压缩效率会比静态模型更高。主流视频编码标准如H.264/H.265都使用自适应模型。
算术编码 vs 哈夫曼编码 首先说结论，算术编码压缩效率更高，哈夫曼编码复杂度更低。
这两种编码，或者说熵编码的本质是，概率越小的字符，用更多的 bit 去表示，这反映到概率区间上就是，概率小的字符所对应的区间也小，因此这个区间的上下边际值的差值越小，为了唯一确定当前这个区间，则需要更多的数字去表示它。
哈夫曼编码由于不断地二叉，它的子区间总是 $\frac{1}{2}$ 的幂次方。而算术编码可以做到严格按照概率的大小等比例划分子区间。所以哈夫曼编码只是算术编码一种粗略的近似。
CABAC CABAC（Context-based Adaptive Binary Arithmetic Coding），CABAC 被视频标准H.264/H.265所采用。
CABAC可以分为二值化、上下文建模和二进制算术编码三个步骤。
其中上下文建模相当于把整段码流进行了再次的细分，把相同条件下的字符bin（比如块大小/亮度色度/语法元素/扫描方式/周围情况等）归属于某个context，形成一个比较独立的子队列而进行编码，其更新只与当前的状态和当前字符是否MPS有关（换句话说，只和历史该子队列编码字符和当前字符有关），而与别的子队列/字符是无关的。当然输出码字往往是根据规则而“混”在一起的。
CABAC虽然性能很好，但也存在以下几点不足：
复杂度过高，不易并行处理。存在块级依赖（左/上角的块没有码率估计/熵编码，后继块就无法得到更新后的状态，从而无法开始码率估计/熵编码）、Bin级依赖（同一个子队列的bin存在前后依赖性，后继的bin要等前面bin编完后才能得到更新后的上下文状态）以及编码的几个环节依赖，这些依赖性会影响编码器的并行实现。 计算精度问题。为简化计算，CABAC采用128个状态来近似，根据原来状态和当前符号性质查表得到下个状态。这个过程中会有一些精度的损失。另外，如果当一连串的MPS到来，状态到达62后就不会继续改变，只会“原地踏步”。换句话说，当概率到达0.01975时就不会随着符号继续变小，这样会影响压缩效率。 Context的设计问题。部分context利用频率很低，在测试中平均一帧都用不到几次，而有的context使用频率很高，需要进一步的优化。 区间编码 (Range Coding) 区间编码可以看为算术编码的一个变种，比算术编码压缩效率略小，但运算速度近乎是算术编码的两倍。
区间编码在整数（任意底）空间中进行进行计算，而算术编码的区间总是以小数的形式进行迭代。其他部分都几乎一样。
端到端熵模型 Todo&amp;hellip;</description>
    </item>
    
    <item>
      <title>图像质量评价指标(MSE, PSNR, MS-SSIM)</title>
      <link>https://imfaye.me/post/image-quality-evaluation-metrics/</link>
      <pubDate>Thu, 02 Dec 2021 23:35:47 +0000</pubDate>
      
      <guid>https://imfaye.me/post/image-quality-evaluation-metrics/</guid>
      <description>如何评价重建图像的质量：比较重建图像与原始图像的可视误差。
MSE Mean Squared Error, 均方误差
$MSE = \frac{1}{N}\sum\limits_{i=1}^{N}(x_i-y_i)^2$
两者越接近，MSE 越小。MSE 损失的范围为 0 到 ∞。
PSNR Peak Signal to Noise Ratio，峰值信噪比，即峰值信号的能量与噪声的平均能量之比，通常取 log 单位为分贝。
$PSNR = 10 log_{10}\frac{MaxValue^2}{MSE}$
从式子可以看出 PSNR 可以理解为 MSE 的另一种表达形式。与 MSE 相反的是，重建图像质量越好，PSNR 数值越大。
对于图像来说，像素点数值以量化方式保存，八比特位深的情况，取值范围为 [0, 255]，$MaxValue$ 就是 255。
SSIM MSE 与 PSNR 的问题是，在计算每个位置上的像素差异时，其结果仅与当前位置的两个像素值有关，与其它任何位置上的像素无关。这种计算差异的方式仅仅将图像看成了一个个孤立的像素点，而忽略了图像内容所包含的一些视觉特征，特别是图像的局部结构信息。而图像质量的好坏极大程度上是一个主观感受，其中结构信息对人主观感受的影响非常之大。
而 SSIM (Structural Similarity，结构相似性) 就试图解决这个问题
SSIM 由三部分组成：
亮度对比 平均灰度作为亮度测量： $\mu_x = \frac{1}{N}\sum\limits_{i=1}^{N}x_i$ 亮度对比函数： $l(x,y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2+\mu_y^2+C_1}$ 对比度对比 灰度标准差作为对比度测量： $\sigma_x={(\frac{1}{N-1}\sum\limits_{i=1}^N{(x_i-\mu_x)}^2)}^{\frac{1}{2}}$ 亮度对比函数： $c(x,y)=\frac{2\sigma_x\sigma_y+C_2}{\sigma_x^2+\sigma_y^2+C_2}$ 结构对比 结构测量： $\frac{x-\mu_x}{\sigma_x}$ 结构对比函数： $s(x,y) = \frac{\sigma_{xy}+C_3}{\sigma_x\sigma_y + C_3}$ SSIM 函数：</description>
    </item>
    
  </channel>
</rss>
